{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "gMLVY3zd802S"
   },
   "source": [
    "### IMPORT REQUIRED LIBRARIES\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GfD36lDc802N"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import random\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6HcbHay802U"
   },
   "source": [
    "### UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnKD75Bm802V"
   },
   "outputs": [],
   "source": [
    "def normalize_weight_matrix(weight_matrix):\n",
    "    \n",
    "    # Applied only while initializing the weight. Later Synaptic scalling applied on weight matrices\n",
    "    \n",
    "    \"\"\" Normalize the weights in the matrix such that incoming connections to a neuron sum up to 1\n",
    "    \n",
    "    Args:\n",
    "        weight_matrix(array) -- Incoming Weights from W_ee or W_ei or W_ie\n",
    "    \n",
    "    Returns:\n",
    "        weight_matrix(array) -- Normalized weight matrix\"\"\"\n",
    "\n",
    "    normalized_weight_matrix = weight_matrix / np.sum(weight_matrix,axis = 0)\n",
    "\n",
    "    return normalized_weight_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6N2LuY9c27YO"
   },
   "source": [
    "### Implement lambda incoming connections for Excitatory neurons and outgoing connections per Inhibitory neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KdS96s3-802Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_lambd_connections(synaptic_connection,ne,ni, lambd_w,lambd_std):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "    synaptic_connection -  Type of sysnpatic connection (EE,EI or IE)\n",
    "    ne - Number of excitatory units\n",
    "    ni - Number of inhibitory units\n",
    "    lambd_w - Average number of incoming connections\n",
    "    lambd_std - Standard deviation of average number of connections per neuron\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    connection_weights - Weight matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if synaptic_connection == 'EE':\n",
    "        \n",
    "        \n",
    "        \"\"\"Choose random lamda connections per neuron\"\"\"\n",
    "\n",
    "        # Draw normally distribued ne integers with mean lambd_w\n",
    "\n",
    "        lambdas_incoming = norm.ppf(np.random.random(ne), loc=lambd_w, scale=lambd_std).astype(int)\n",
    "        \n",
    "        # lambdas_outgoing = norm.ppf(np.random.random(ne), loc=lambd_w, scale=lambd_std).astype(int)\n",
    "    \n",
    "        # List of neurons \n",
    "\n",
    "        list_neurons= list(range(ne))\n",
    "\n",
    "        # Connection weights\n",
    "\n",
    "        connection_weights = np.zeros((ne,ne))\n",
    "\n",
    "        # For each lambd value in the above list,\n",
    "        # generate weights for incoming and outgoing connections\n",
    "            \n",
    "        #-------------Gaussian Distribution of weights --------------\n",
    "            \n",
    "        # weight_matrix = np.random.randn(Sorn.ne, Sorn.ni) + 2 # Small random values from gaussian distribution\n",
    "                                                                # Centered around 2 to make all values positive \n",
    "            \n",
    "        # ------------Uniform Distribution --------------------------\n",
    "        global_incoming_weights = np.random.uniform(0.0,0.1,sum(lambdas_incoming))\n",
    "        \n",
    "        # Index Counter\n",
    "        global_incoming_weights_idx = 0\n",
    "        \n",
    "        # Choose the neurons in order [0 to 199]\n",
    "        \n",
    "        for neuron in list_neurons:\n",
    "\n",
    "            ### Choose ramdom unique (lambdas[neuron]) neurons from  list_neurons\n",
    "            possible_connections = list_neurons.copy()\n",
    "            \n",
    "            possible_connections.remove(neuron)  # Remove the selected neuron from possible connections i!=j\n",
    "            \n",
    "            # Choose random presynaptic neurons\n",
    "            possible_incoming_connections = random.sample(possible_connections,lambdas_incoming[neuron])  \n",
    "\n",
    "           \n",
    "            incoming_weights_neuron = global_incoming_weights[global_incoming_weights_idx:global_incoming_weights_idx+lambdas_incoming[neuron]]\n",
    "            \n",
    "            # ---------- Update the connection weight matrix ------------\n",
    "\n",
    "            # Update incoming connection weights for selected 'neuron'\n",
    "\n",
    "            for incoming_idx,incoming_weight in enumerate(incoming_weights_neuron):  \n",
    "                connection_weights[possible_incoming_connections[incoming_idx]][neuron] = incoming_weight\n",
    "            \n",
    "            global_incoming_weights_idx += lambdas_incoming[neuron]\n",
    "        \n",
    "        return connection_weights\n",
    "    \n",
    "    if synaptic_connection == 'EI':\n",
    "        \n",
    "        \"\"\"Choose random lamda connections per neuron\"\"\"\n",
    "\n",
    "        # Draw normally distribued ni integers with mean lambd_w\n",
    "        lambdas = norm.ppf(np.random.random(ni), loc=lambd_w, scale=lambd_std).astype(int)\n",
    "        \n",
    "        # List of neurons \n",
    "\n",
    "        list_neurons= list(range(ni))  # Each i can connect with random ne neurons \n",
    "\n",
    "        # Initializing connection weights variable\n",
    "\n",
    "        connection_weights = np.zeros((ni,ne))\n",
    "\n",
    "        # ------------Uniform Distribution -----------------------------\n",
    "        global_outgoing_weights = np.random.uniform(0.0,0.1,sum(lambdas))\n",
    "        \n",
    "        # Index Counter\n",
    "        global_outgoing_weights_idx = 0\n",
    "        \n",
    "        # Choose the neurons in order [0 to 40]\n",
    "\n",
    "        for neuron in list_neurons:\n",
    "\n",
    "            ### Choose ramdom unique (lambdas[neuron]) neurons from  list_neurons\n",
    "            possible_connections = list(range(ne))\n",
    "            \n",
    "            possible_outgoing_connections = random.sample(possible_connections,lambdas[neuron])  # possible_outgoing connections to the neuron\n",
    "\n",
    "            # Update weights\n",
    "            outgoing_weights = global_outgoing_weights[global_outgoing_weights_idx:global_outgoing_weights_idx+lambdas[neuron]]\n",
    "\n",
    "            # ---------- Update the connection weight matrix ------------\n",
    "\n",
    "            # Update outgoing connections for the neuron\n",
    "\n",
    "            for outgoing_idx,outgoing_weight in enumerate(outgoing_weights):  # Update the columns in the connection matrix\n",
    "                connection_weights[neuron][possible_outgoing_connections[outgoing_idx]] = outgoing_weight\n",
    "            \n",
    "            # Update the global weight values index\n",
    "            global_outgoing_weights_idx += lambdas[neuron]\n",
    "            \n",
    "        \n",
    "        return connection_weights\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwA0K5_u4nkG"
   },
   "source": [
    "### More Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bturycg4802d"
   },
   "outputs": [],
   "source": [
    "def get_incoming_connection_dict(weights):\n",
    "    \n",
    "    # Get the non-zero entires in columns is the incoming connections for the neurons\n",
    "    \n",
    "    # Indices of nonzero entries in the columns\n",
    "    connection_dict=dict.fromkeys(range(1,len(weights)+1),0)\n",
    "    \n",
    "    for i in range(len(weights[0])):  # For each neuron\n",
    "        connection_dict[i] = list(np.nonzero(weights[:,i])[0])\n",
    "        \n",
    "    return connection_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b0l1Ly2g802h"
   },
   "outputs": [],
   "source": [
    "def get_outgoing_connection_dict(weights):\n",
    "    # Get the non-zero entires in rows is the outgoing connections for the neurons\n",
    "    \n",
    "    # Indices of nonzero entries in the rows\n",
    "    connection_dict=dict.fromkeys(range(1,len(weights)+1),1)\n",
    "    \n",
    "    for i in range(len(weights[0])):  # For each neuron\n",
    "        connection_dict[i] = list(np.nonzero(weights[i,:])[0])\n",
    "        \n",
    "    return connection_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YScmyJa802l"
   },
   "outputs": [],
   "source": [
    "def prune_small_weights(weights,cutoff_weight):\n",
    "    \n",
    "    \"\"\" Prune the connections with negative connection strength\"\"\"\n",
    "    weights[weights <= cutoff_weight] = cutoff_weight\n",
    "    \n",
    "    return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nuGuabFp802p"
   },
   "outputs": [],
   "source": [
    "def set_max_cutoff_weight(weights, cutoff_weight):\n",
    "    \n",
    "    \"\"\" Set cutoff limit for the values in given array\"\"\"\n",
    "    \n",
    "    weights[weights > cutoff_weight] = cutoff_weight\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IR2-66u6802t"
   },
   "outputs": [],
   "source": [
    "def get_unconnected_indexes(wee):\n",
    "    \n",
    "    \"\"\"\n",
    "    Helper function for Structural plasticity to randomly select the unconnected units\n",
    "    \n",
    "    Args: \n",
    "    wee -  Weight matrix\n",
    "    \n",
    "    Returns:\n",
    "    list (indices) // indices = (row_idx,col_idx)\"\"\"\n",
    "    \n",
    "\n",
    "    i,j = np.where(wee <= 0.)\n",
    "    indices = list(zip(i,j))\n",
    "    \n",
    "    self_conn_removed = []\n",
    "    for i,idxs in enumerate(indices):\n",
    "        \n",
    "        if idxs[0] != idxs[1]:\n",
    "            \n",
    "            self_conn_removed.append(indices[i])\n",
    "    \n",
    "    return self_conn_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1qctjLG802x"
   },
   "outputs": [],
   "source": [
    "def white_gaussian_noise(mu, sigma,t):\n",
    "\n",
    "    \"\"\"Generates white gaussian noise with mean mu, standard deviation sigma and\n",
    "    the noise length equals t \"\"\"\n",
    "    \n",
    "    noise = np.random.normal(mu, sigma, t)   \n",
    "    \n",
    "    return np.expand_dims(noise,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O2lmxtlC8024"
   },
   "outputs": [],
   "source": [
    "### SANITY CHECK EACH WEIGHTS\n",
    "#### Note this function has no influence in weight matrix, will be deprecated in next version\n",
    "\n",
    "def zero_sum_incoming_check(weights):\n",
    "    \n",
    "    zero_sum_incomings = np.where(np.sum(weights,axis = 0) == 0.)\n",
    "    \n",
    "    if len(zero_sum_incomings[-1]) == 0:\n",
    "        return weights\n",
    "    else:\n",
    "        for zero_sum_incoming in zero_sum_incomings[-1]:\n",
    "            \n",
    "            rand_indices = np.random.randint(40,size = 2)  # 5 because each excitatory neuron connects with 5 inhibitory neurons \n",
    "                                                            # given the probability of connections 0.2\n",
    "            rand_values = np.random.uniform(0.0,0.1,2)\n",
    "            \n",
    "            for i,idx in enumerate(rand_indices):\n",
    "                \n",
    "                weights[:,zero_sum_incoming][idx] = rand_values[i]\n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7lA_nOd803A"
   },
   "source": [
    "### SORN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ukB19wse803C"
   },
   "outputs": [],
   "source": [
    "class Sorn(object):\n",
    "    \n",
    "    \"\"\"SORN 1 network model Initialization\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\"Initialize network variables as class variables of SORN\"\"\"\n",
    "    \n",
    "    nu = 10                    # Number of input units\n",
    "    ne = 200                   # Number of excitatory units\n",
    "    ni = int(0.2*ne)           # Number of inhibitory units in the network\n",
    "    eta_stdp = 0.004\n",
    "    eta_inhib = 0.001\n",
    "    eta_ip = 0.01\n",
    "    te_max = 1.0 \n",
    "    ti_max = 0.5\n",
    "    ti_min = 0.0\n",
    "    te_min = 0.0\n",
    "    mu_ip = 0.1\n",
    "    sigma_ip  = 0.0 # Standard deviation, variance == 0 \n",
    "    \n",
    "    \n",
    "    # Initialize weight matrices\n",
    "\n",
    "    def initialize_weight_matrix(self, network_type,synaptic_connection, self_connection, lambd_w): \n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "        \n",
    "        network_type(str) - Spare or Dense\n",
    "        synaptic_connection(str) - EE,EI,IE: Note that Spare connection is defined only for EE connections\n",
    "        self_connection(str) - True or False: i-->i ; Network is tested only using j-->i\n",
    "        lambd_w(int) - Average number of incoming and outgoing connections per neuron\n",
    "        \n",
    "        Returns:\n",
    "        weight_matrix(array) -  Array of connection strengths \n",
    "        \"\"\"\n",
    "        \n",
    "        if (network_type == \"Sparse\") and (self_connection == \"False\"):\n",
    "\n",
    "            \"\"\"Generate weight matrix for E-E/ E-I connections with mean lamda incoming and outgiong connections per neuron\"\"\"\n",
    "            \n",
    "            weight_matrix = generate_lambd_connections(synaptic_connection,Sorn.ne,Sorn.ni,lambd_w,lambd_std = 1)\n",
    "        \n",
    "        # Dense matrix for W_ie\n",
    "\n",
    "        elif (network_type == 'Dense') and (self_connection == 'False'):\n",
    "\n",
    "            # Gaussian distribution of weights\n",
    "            # weight_matrix = np.random.randn(Sorn.ne, Sorn.ni) + 2 # Small random values from gaussian distribution\n",
    "            # Centered around 1 \n",
    "            # weight_matrix.reshape(Sorn.ne, Sorn.ni) \n",
    "            # weight_matrix *= 0.01 # Setting spectral radius \n",
    "            \n",
    "            # Uniform distribution of weights\n",
    "            weight_matrix = np.random.uniform(0.0,0.1,(Sorn.ne, Sorn.ni))\n",
    "            weight_matrix.reshape((Sorn.ne,Sorn.ni))\n",
    "\n",
    "        return weight_matrix\n",
    "\n",
    "    def initialize_threshold_matrix(self, te_min,te_max, ti_min,ti_max):\n",
    "\n",
    "        # Initialize the threshold for excitatory and inhibitory neurons\n",
    "        \n",
    "        \"\"\"Args:\n",
    "            te_min(float) -- Min threshold value for excitatory units\n",
    "            ti_min(float) -- Min threshold value for inhibitory units\n",
    "            te_max(float) -- Max threshold value for excitatory units\n",
    "            ti_max(float) -- Max threshold value for inhibitory units\n",
    "        Returns:\n",
    "            te(vector) -- Threshold values for excitatory units\n",
    "            ti(vector) -- Threshold values for inhibitory units\"\"\"\n",
    "\n",
    "        te = np.random.uniform(0., te_max, (Sorn.ne, 1))\n",
    "        ti = np.random.uniform(0., ti_max, (Sorn.ni, 1))\n",
    "\n",
    "        return te, ti\n",
    "\n",
    "    def initialize_activity_vector(self,ne, ni):\n",
    "        \n",
    "        # Initialize the activity vectors X and Y for excitatory and inhibitory neurons\n",
    "        \n",
    "        \"\"\"Args:\n",
    "            ne(int) -- Number of excitatory neurons\n",
    "            ni(int) -- Number of inhibitory neurons\n",
    "        Returns:\n",
    "             x(array) -- Array of activity vectors of excitatory population\n",
    "             y(array) -- Array of activity vectors of inhibitory population\"\"\"\n",
    "\n",
    "        x = np.zeros((ne, 2))\n",
    "        y = np.zeros((ni, 2))\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MDrcQ0w803d"
   },
   "outputs": [],
   "source": [
    "class Plasticity(Sorn):\n",
    "    \"\"\"\n",
    "    Instance of class Sorn. Inherits the variables and functions defined in class Sorn\n",
    "    Encapsulates all plasticity mechanisms mentioned in the article \"\"\"\n",
    "\n",
    "    # Initialize the global variables for the class //Class attributes\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.nu = Sorn.nu                  # Number of input units\n",
    "        self.ne = Sorn.ne                  # Number of excitatory units\n",
    "        self.eta_stdp = Sorn.eta_stdp      # STDP plasticity Learning rate constant; SORN1 and SORN2\n",
    "        self.eta_ip = Sorn.eta_ip          # Intrinsic plasticity learning rate constant; SORN1 and SORN2\n",
    "        self.eta_inhib = Sorn.eta_inhib    # Intrinsic plasticity learning rate constant; SORN2 only\n",
    "        self.h_ip = 2 * Sorn.nu / Sorn.ne  # Target firing rate\n",
    "        self.mu_ip = Sorn.mu_ip            # Mean target firing rate  \n",
    "        self.ni = Sorn.ni                  # Number of inhibitory units in the network\n",
    "        self.time_steps = Sorn.time_steps  # Total time steps of simulation\n",
    "        self.te_min = Sorn.te_min          # Excitatory minimum Threshold\n",
    "        self.te_max = Sorn.te_max          # Excitatory maximum Threshold\n",
    "        \n",
    "    def stdp(self, wee, x, cutoff_weights):\n",
    "        \n",
    "        \"\"\" Apply STDP rule : Regulates synaptic strength between the pre(Xj) and post(Xi) synaptic neurons\"\"\"\n",
    "\n",
    "        x = np.asarray(x)\n",
    "        xt_1 = x[:,0]\n",
    "        xt = x[:,1]\n",
    "        wee_t = wee.copy()\n",
    "        \n",
    "        # STDP applies only on the neurons which are connected.\n",
    "        \n",
    "        for i in range(len(wee_t[0])): # Each neuron i, Post-synaptic neuron\n",
    "            \n",
    "            for j in range(len(wee_t[0:])): # Incoming connection from jth pre-synaptic neuron to ith neuron\n",
    "                \n",
    "                if wee_t[j][i] != 0. : # Check connectivity\n",
    "                    \n",
    "                    # Get the change in weight\n",
    "                    delta_wee_t = self.eta_stdp * (xt[i] * xt_1[j] - xt_1[i]*xt[j])\n",
    "\n",
    "                    # Update the weight between jth neuron to i \"\"Different from notation in article \n",
    "\n",
    "                    wee_t[j][i] = wee[j][i] + delta_wee_t\n",
    "        \n",
    "        \"\"\" Prune the smallest weights induced by plasticity mechanisms; Apply lower cutoff weight\"\"\"\n",
    "        wee_t = prune_small_weights(wee_t,cutoff_weights[0])\n",
    "        \n",
    "        \"\"\"Check and set all weights < upper cutoff weight \"\"\"\n",
    "        wee_t = set_max_cutoff_weight(wee_t,cutoff_weights[1])\n",
    "\n",
    "        return wee_t\n",
    "\n",
    "    def ip(self, te, x):\n",
    "        \n",
    "        # IP rule: Active unit increases its threshold and inactive decreases its threshold.\n",
    "\n",
    "        xt = x[:, 1]\n",
    "\n",
    "        te_update = te + self.eta_ip * (xt.reshape(self.ne, 1) - self.h_ip)\n",
    "        \n",
    "        \"\"\" Check whether all te are in range [0.0,1.0] and update acordingly\"\"\"\n",
    "        \n",
    "        # Update te < 0.0 ---> 0.0\n",
    "        # te_update = prune_small_weights(te_update,self.te_min)\n",
    "        \n",
    "        # Set all te > 1.0 --> 1.0\n",
    "        # te_update = set_max_cutoff_weight(te_update,self.te_max)\n",
    "\n",
    "        return te_update\n",
    "\n",
    "    def ss(self, wee_t):\n",
    "        \n",
    "        \"\"\"Synaptic Scaling or Synaptic Normalization\"\"\"\n",
    "        \n",
    "        wee_t = wee_t / np.sum(wee_t,axis=0)\n",
    "\n",
    "        return wee_t\n",
    "\n",
    "    \n",
    "    def istdp(self, wei, x, y, cutoff_weights):\n",
    "\n",
    "        #  Apply iSTDP rule : Regulates synaptic strength between the pre(Yj) and post(Xi) synaptic neurons\n",
    "        \n",
    "        # Excitaotry network activity\n",
    "        x = np.asarray(x) # Array sanity check\n",
    "        xt_1 = x[:, 0]  \n",
    "        xt = x[:, 1]  \n",
    "    \n",
    "        # Inhibitory network activity\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        yt_1 = y[:, 0]    \n",
    "        yt = y[:, 1]  \n",
    " \n",
    "\n",
    "        # iSTDP applies only on the neurons which are connected.\n",
    "        wei_t = wei.copy()\n",
    "\n",
    "        for i in range(len(wei_t[0])): # Each neuron i, Post-synaptic neuron: means for each column; \n",
    "            \n",
    "            for j in range(len(wei_t[0:])): # Incoming connection from j, pre-synaptic neuron to ith neuron\n",
    "                \n",
    "                if wei_t[j][i] != 0. : # Check connectivity\n",
    "                    \n",
    "                    # Get the change in weight\n",
    "                    delta_wei_t = - self.eta_inhib * yt_1[j] * (1 - xt[i]*(1 + 1/self.mu_ip))\n",
    "\n",
    "                    # Update the weight between jth neuron to i \"\"Different from notation in article \n",
    "\n",
    "                    wei_t[j][i] = wei[j][i] + delta_wei_t\n",
    "        \n",
    "        \"\"\" Prune the smallest weights induced by plasticity mechanisms; Apply lower cutoff weight\"\"\"\n",
    "        wei_t = prune_small_weights(wei_t,cutoff_weights[0])\n",
    "        \n",
    "        \"\"\"Check and set all weights < upper cutoff weight \"\"\"\n",
    "        wei_t = set_max_cutoff_weight(wei_t,cutoff_weights[1])\n",
    "        \n",
    "        return wei_t\n",
    "\n",
    "    @staticmethod\n",
    "    def structural_plasticity(wee):\n",
    "\n",
    "        \"\"\" Add new connection value to the smallest weight between excitatory units randomly\"\"\"\n",
    "\n",
    "        p_c = np.random.randint(0, 10, 1)\n",
    "\n",
    "        if p_c == 0:  # p_c= 0.1\n",
    "\n",
    "            \"\"\" Do structural plasticity \"\"\"\n",
    "\n",
    "            # Choose the smallest weights randomly from the weight matrix wee\n",
    "            \n",
    "            indexes = get_unconnected_indexes(wee) \n",
    "\n",
    "            # Choose any idx randomly\n",
    "            idx_rand = random.choice(indexes)\n",
    "            \n",
    "            if idx_rand[0] == idx_rand[1]:\n",
    "                \n",
    "                idx_rand = random.choice(indexes)\n",
    "                \n",
    "            wee[idx_rand[0]][idx_rand[1]] = 0.001\n",
    "            \n",
    "\n",
    "        return wee\n",
    "\n",
    "    ###########################################################\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_plasticity():\n",
    "\n",
    "        wee = wee_init\n",
    "        wei = wei_init\n",
    "        wie = wie_init\n",
    "        te = te_init\n",
    "        ti = ti_init\n",
    "        x = x_init\n",
    "        y = y_init\n",
    "\n",
    "        return wee, wei, wie, te, ti, x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def reorganize_network():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6RrY8CIS803j"
   },
   "outputs": [],
   "source": [
    "class MatrixCollection(Sorn):\n",
    "    def __init__(self,phase, matrices = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.phase = phase\n",
    "        self.matrices = matrices\n",
    "        if self.phase == 'Plasticity' and self.matrices == None :\n",
    "\n",
    "            self.time_steps = Sorn.time_steps + 1  # Total training steps\n",
    "            self.Wee, self.Wei, self.Wie, self.Te, self.Ti, self.X, self.Y = [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps\n",
    "            wee, wei, wie, te, ti, x, y = Plasticity.initialize_plasticity()\n",
    "\n",
    "            # Assign initial matrix to the master matrices\n",
    "            self.Wee[0] = wee\n",
    "            self.Wei[0] = wei\n",
    "            self.Wie[0] = wie\n",
    "            self.Te[0] = te\n",
    "            self.Ti[0] = ti\n",
    "            self.X[0] = x\n",
    "            self.Y[0] = y\n",
    "        \n",
    "        elif self.phase == 'Plasticity' and self.matrices != None:\n",
    "            \n",
    "            self.time_steps = Sorn.time_steps + 1  # Total training steps\n",
    "            self.Wee, self.Wei, self.Wie, self.Te, self.Ti, self.X, self.Y = [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps\n",
    "            # Assign matrices from plasticity phase to the new master matrices for training phase\n",
    "            self.Wee[0] = matrices['Wee']\n",
    "            self.Wei[0] = matrices['Wei']\n",
    "            self.Wie[0] = matrices['Wie']\n",
    "            self.Te[0] = matrices['Te']\n",
    "            self.Ti[0] = matrices['Ti']\n",
    "            self.X[0] = matrices['X']\n",
    "            self.Y[0] = matrices['Y']\n",
    "            \n",
    "        elif self.phase == 'Training':\n",
    "\n",
    "            \"\"\"NOTE:\n",
    "            time_steps here is diferent for plasticity or trianing phase\"\"\"\n",
    "            self.time_steps = Sorn.time_steps + 1  # Total training steps\n",
    "            self.Wee, self.Wei, self.Wie, self.Te, self.Ti, self.X, self.Y = [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps\n",
    "            # Assign matrices from plasticity phase to new respective matrices for training phase\n",
    "            self.Wee[0] = matrices['Wee']\n",
    "            self.Wei[0] = matrices['Wei']\n",
    "            self.Wie[0] = matrices['Wie']\n",
    "            self.Te[0] = matrices['Te']\n",
    "            self.Ti[0] = matrices['Ti']\n",
    "            self.X[0] = matrices['X']\n",
    "            self.Y[0] = matrices['Y']\n",
    "            \n",
    "    # @staticmethod\n",
    "    def weight_matrix(self, wee, wei, wie, i):\n",
    "        # Get delta_weight from Plasticity.stdp\n",
    "    \n",
    "        # i - training step\n",
    "        self.Wee[i + 1] = wee\n",
    "        self.Wei[i + 1] = wei\n",
    "        self.Wie[i + 1] = wie\n",
    "\n",
    "        return self.Wee, self.Wei, self.Wie\n",
    "\n",
    "    # @staticmethod\n",
    "    def threshold_matrix(self, te, ti, i):\n",
    "        self.Te[i + 1] = te\n",
    "        self.Ti[i + 1] = ti\n",
    "        return self.Te, self.Ti\n",
    "\n",
    "    # @staticmethod\n",
    "    def network_activity_t(self, excitatory_net, inhibitory_net, i):\n",
    "        self.X[i + 1] = excitatory_net\n",
    "        self.Y[i + 1] = inhibitory_net\n",
    "\n",
    "        return self.X, self.Y\n",
    "\n",
    "    # @staticmethod\n",
    "    def network_activity_t_1(self, x, y, i):\n",
    "        x_1, y_1 = [0] * self.time_steps, [0] * self.time_steps\n",
    "        x_1[i] = x\n",
    "        y_1[i] = y\n",
    "\n",
    "        return x_1, y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XnPxhhDj803p"
   },
   "outputs": [],
   "source": [
    "class NetworkState(Plasticity):\n",
    "    \n",
    "    \"\"\"The evolution of network states\"\"\"\n",
    "\n",
    "    def __init__(self, v_t):\n",
    "        super().__init__()\n",
    "        self.v_t = v_t\n",
    "    \n",
    "    def incoming_drive(self,weights,activity_vector):\n",
    "            \n",
    "        # Broadcasting weight*acivity vectors \n",
    "        \n",
    "        incoming = weights* activity_vector\n",
    "        incoming = np.array(incoming.sum(axis=0))\n",
    "        return incoming\n",
    "        \n",
    "    def excitatory_network_state(self, wee, wei, te, x, y,white_noise_e):\n",
    "        \n",
    "        \"\"\" Activity of Excitatory neurons in the network\"\"\"\n",
    "    \n",
    "        xt = x[:, 1]  \n",
    "        xt = xt.reshape(self.ne, 1)\n",
    "        yt = y[:, 1]\n",
    "        yt = yt.reshape(self.ni, 1)\n",
    "        \n",
    "        \n",
    "        incoming_drive_e = np.expand_dims(self.incoming_drive(weights = wee,activity_vector=xt),1)\n",
    "        incoming_drive_i = np.expand_dims(self.incoming_drive(weights = wei,activity_vector=yt),1)\n",
    "        tot_incoming_drive = incoming_drive_e -  incoming_drive_i + white_noise_e + np.expand_dims(np.asarray(self.v_t),1) - te\n",
    "        \n",
    "        \"\"\"Heaviside step function\"\"\"\n",
    "\n",
    "        heaviside_step = [0] * len(tot_incoming_drive)\n",
    "        for t in range(len(tot_incoming_drive)):\n",
    "            heaviside_step[t] = 0.0 if tot_incoming_drive[t] < te[t] else 1.0\n",
    "\n",
    "        xt_next = np.asarray(heaviside_step.copy())\n",
    "\n",
    "        return xt_next\n",
    "\n",
    "    def inhibitory_network_state(self, wie, ti, x,white_noise_i):\n",
    "\n",
    "        # Activity of inhibitory neurons\n",
    "\n",
    "        wie = np.asarray(wie)\n",
    "        xt = x[:, 1]\n",
    "        xt = xt.reshape(Sorn.ne, 1)\n",
    "    \n",
    "        incoming_drive_e = np.expand_dims(self.incoming_drive(weights = wie,activity_vector=xt),1)\n",
    "        \n",
    "        tot_incoming_drive = incoming_drive_e + white_noise_i - ti\n",
    "        \n",
    "        \"\"\"Implement Heaviside step function\"\"\"\n",
    "\n",
    "        heaviside_step = [0] * len(tot_incoming_drive)\n",
    "\n",
    "        for t in range(len(tot_incoming_drive)):\n",
    "            heaviside_step[t] = 0.0 if tot_incoming_drive[t] < ti[t] else 1.0\n",
    "\n",
    "        yt_next = np.asarray(heaviside_step.copy())  \n",
    "\n",
    "        return yt_next\n",
    "\n",
    "    \n",
    "    def recurrent_drive(self, wee, wei, te, x, y,white_noise_e):\n",
    "        \n",
    "        \"\"\"Network state due to recurrent drive received by the each unit at time t+1\"\"\"\n",
    "        \n",
    "    \n",
    "        xt = x[:, 1]  \n",
    "        xt = xt.reshape(self.ne, 1)\n",
    "        yt = y[:, 1]\n",
    "        yt = yt.reshape(self.ni, 1)\n",
    "        \n",
    "        incoming_drive_e = np.expand_dims(self.incoming_drive(weights = wee,activity_vector=xt),1)\n",
    "        incoming_drive_i = np.expand_dims(self.incoming_drive(weights = wei,activity_vector=yt),1)\n",
    "        \n",
    "        tot_incoming_drive = incoming_drive_e -  incoming_drive_i + white_noise_e - te\n",
    "        \n",
    "        \"\"\"Heaviside step function\"\"\"\n",
    "\n",
    "        heaviside_step = [0] * len(tot_incoming_drive)\n",
    "        for t in range(len(tot_incoming_drive)):\n",
    "            heaviside_step[t] = 0.0 if tot_incoming_drive[t] < te[t] else 1.0\n",
    "\n",
    "        xt_next = np.asarray(heaviside_step.copy())\n",
    "\n",
    "        return xt_next\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper class for training SORN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainSornPlasticity(Sorn):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        Args:\n",
    "        inputs - one hot vector of inputs\n",
    "    \n",
    "        Returns:\n",
    "        matrix_collection - collection of all weight matrices in dictionaries\n",
    "        \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        pass\n",
    "        \n",
    "\n",
    "    \n",
    "    def train_sorn(self,phase,matrices,inputs):\n",
    "        \n",
    "        self.phase = phase\n",
    "        self.time_steps = 1\n",
    "        Sorn.time_steps = 1\n",
    "        self.inputs = np.asarray(inputs)\n",
    "        self.phase = phase\n",
    "        self.matrices = matrices\n",
    "        \n",
    "        # Collect the network activity at all time steps\n",
    "        \n",
    "        X_all = [0]*self.time_steps\n",
    "        Y_all = [0]*self.time_steps\n",
    "        R_all = [0]*self.time_steps\n",
    "        \n",
    "        frac_pos_active_conn = []\n",
    "         \n",
    "        \"\"\" DONOT INITIALIZE WEIGHTS\"\"\"\n",
    "        \n",
    "        matrix_collection = MatrixCollection(phase = self.phase, matrices = self.matrices)  \n",
    "        \n",
    "        for i in range(1):\n",
    "            \n",
    "            \"\"\" Generate white noise\"\"\"\n",
    "            # white_noise_e = white_gaussian_noise(mu= 0., sigma = 0.04,t = Sorn.ne)\n",
    "            # white_noise_i = white_gaussian_noise(mu= 0., sigma = 0.04,t = Sorn.ni)\n",
    "            \n",
    "            network_state = NetworkState(self.inputs.T)  # Feed Input as an argument to the class\n",
    "            \n",
    "\n",
    "            # Buffers to get the resulting x and y vectors at the current time step and update the master matrix\n",
    "\n",
    "            x_buffer, y_buffer = np.zeros(( Sorn.ne, 2)), np.zeros((Sorn.ni, 2))\n",
    "\n",
    "            te_buffer, ti_buffer = np.zeros((Sorn.ne, 1)), np.zeros((Sorn.ni, 1))\n",
    "\n",
    "            # Get the matrices and rename them for ease of reading\n",
    "\n",
    "            Wee, Wei, Wie = matrix_collection.Wee, matrix_collection.Wei, matrix_collection.Wie\n",
    "            Te, Ti = matrix_collection.Te, matrix_collection.Ti\n",
    "            X, Y = matrix_collection.X, matrix_collection.Y\n",
    "            \n",
    "            \n",
    "            # Recurrent drive at t+1 used to predict the next external stimuli\n",
    "            \n",
    "            r = network_state.recurrent_drive(Wee[i], Wei[i], Te[i], X[i], Y[i],white_noise_e = 0.)\n",
    "\n",
    "            \"\"\"Get excitatory states and inhibitory states given the weights and thresholds\"\"\"\n",
    "\n",
    "            # x(t+1), y(t+1)\n",
    "            excitatory_state_xt_buffer = network_state.excitatory_network_state(Wee[i], Wei[i], Te[i], X[i], Y[i],white_noise_e = 0.)\n",
    "\n",
    "            inhibitory_state_yt_buffer = network_state.inhibitory_network_state(Wie[i], Ti[i], X[i],white_noise_i = 0.)\n",
    "            \n",
    "            \n",
    "            \"\"\" Update X and Y \"\"\"\n",
    "            x_buffer[:, 0] = X[i][:, 1]  # xt -->(becomes) xt_1\n",
    "            x_buffer[:, 1] = excitatory_state_xt_buffer.T  # New_activation; x_buffer --> xt\n",
    "            \n",
    "\n",
    "            y_buffer[:, 0] = Y[i][:, 1]\n",
    "            y_buffer[:, 1] = inhibitory_state_yt_buffer.T\n",
    "            \n",
    "\n",
    "            \"\"\"Plasticity phase\"\"\"\n",
    "\n",
    "            plasticity = Plasticity()\n",
    "\n",
    "            # STDP \n",
    "            Wee_t = plasticity.stdp(Wee[i],x_buffer,cutoff_weights = (0.0,1.0))\n",
    "              \n",
    "            # Intrinsic plasticity\n",
    "            Te_t = plasticity.ip(Te[i],x_buffer)\n",
    "              \n",
    "            # Structural plasticity\n",
    "            Wee_t = plasticity.structural_plasticity(Wee_t)      \n",
    "            \n",
    "            # iSTDP \n",
    "            Wei_t = plasticity.istdp(Wei[i],x_buffer,y_buffer,cutoff_weights = (0.0,1.0))\n",
    "            \n",
    "            # Synaptic scaling Wee\n",
    "            Wee_t = Plasticity().ss(Wee_t)\n",
    "            \n",
    "            # Synaptic scaling Wei\n",
    "            Wei_t = Plasticity().ss(Wei_t)\n",
    "\n",
    "            \"\"\"Assign the matrices to the matrix collections\"\"\"\n",
    "            matrix_collection.weight_matrix(Wee_t, Wei_t, Wie[i], i)\n",
    "            matrix_collection.threshold_matrix(Te_t, Ti[i], i)\n",
    "            matrix_collection.network_activity_t(x_buffer, y_buffer, i)\n",
    "            \n",
    "            X_all[i] = x_buffer[:,1]\n",
    "            Y_all[i] = y_buffer[:,1]\n",
    "            R_all[i] = r\n",
    "   \n",
    "        plastic_matrices = {'Wee':matrix_collection.Wee[-1], \n",
    "                            'Wei': matrix_collection.Wei[-1], \n",
    "                            'Wie':matrix_collection.Wie[-1],\n",
    "                            'Te': matrix_collection.Te[-1], 'Ti': matrix_collection.Ti[-1],\n",
    "                            'X': X[-1], 'Y': Y[-1]}\n",
    "        \n",
    "        return plastic_matrices,X_all,Y_all,R_all,frac_pos_active_conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainSorn(Sorn):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        Args:\n",
    "        inputs - one hot vector of inputs\n",
    "    \n",
    "        Returns:\n",
    "        matrix_collection - collection of all weight matrices in dictionaries\n",
    "        \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        pass\n",
    "        \n",
    "\n",
    "    \n",
    "    def train_sorn(self,phase,matrices,inputs):\n",
    "        \n",
    "        self.phase = phase\n",
    "        self.time_steps = 1\n",
    "        Sorn.time_steps = 1\n",
    "        self.inputs = np.asarray(inputs)\n",
    "        self.phase = phase\n",
    "        self.matrices = matrices\n",
    "        \n",
    "        # Collect the network activity at all time steps\n",
    "        \n",
    "        X_all = [0]*self.time_steps\n",
    "        Y_all = [0]*self.time_steps\n",
    "        R_all = [0]*self.time_steps\n",
    "        \n",
    "        frac_pos_active_conn = []\n",
    "         \n",
    "        \"\"\" DONOT INITIALIZE WEIGHTS\"\"\"\n",
    "        \n",
    "        matrix_collection = MatrixCollection(phase = self.phase, matrices = self.matrices)  \n",
    "        \n",
    "        for i in range(1):\n",
    "            \n",
    "            \"\"\" Generate white noise\"\"\"\n",
    "            # white_noise_e = white_gaussian_noise(mu= 0., sigma = 0.04,t = Sorn.ne)\n",
    "            # white_noise_i = white_gaussian_noise(mu= 0., sigma = 0.04,t = Sorn.ni)\n",
    "\n",
    "            network_state = NetworkState(self.inputs.T)  # Feed Input as an argument to the class\n",
    "            \n",
    "\n",
    "            # Buffers to get the resulting x and y vectors at the current time step and update the master matrix\n",
    "\n",
    "            x_buffer, y_buffer = np.zeros(( Sorn.ne, 2)), np.zeros((Sorn.ni, 2))\n",
    "\n",
    "            te_buffer, ti_buffer = np.zeros((Sorn.ne, 1)), np.zeros((Sorn.ni, 1))\n",
    "\n",
    "            # Get the matrices and rename them for ease of reading\n",
    "\n",
    "            Wee, Wei, Wie = matrix_collection.Wee, matrix_collection.Wei, matrix_collection.Wie\n",
    "            Te, Ti = matrix_collection.Te, matrix_collection.Ti\n",
    "            X, Y = matrix_collection.X, matrix_collection.Y\n",
    "            \n",
    "            \n",
    "            # Recurrent drive at t+1 used to predict the next external stimuli\n",
    "            \n",
    "            r = network_state.recurrent_drive(Wee[i], Wei[i], Te[i], X[i], Y[i],white_noise_e = 0.)\n",
    "\n",
    "            \"\"\"Get excitatory states and inhibitory states given the weights and thresholds\"\"\"\n",
    "\n",
    "            # x(t+1), y(t+1)\n",
    "            excitatory_state_xt_buffer = network_state.excitatory_network_state(Wee[i], Wei[i], Te[i], X[i], Y[i],white_noise_e = 0.)\n",
    "\n",
    "            inhibitory_state_yt_buffer = network_state.inhibitory_network_state(Wie[i], Ti[i], X[i],white_noise_i = 0.)\n",
    "            \n",
    "            \n",
    "            \"\"\" Update X and Y \"\"\"\n",
    "            x_buffer[:, 0] = X[i][:, 1]  # xt -->(becomes) xt_1\n",
    "            x_buffer[:, 1] = excitatory_state_xt_buffer.T  # New_activation; x_buffer --> xt\n",
    "            \n",
    "\n",
    "            y_buffer[:, 0] = Y[i][:, 1]\n",
    "            y_buffer[:, 1] = inhibitory_state_yt_buffer.T\n",
    "            \n",
    "\n",
    "            \"\"\"Plasticity phase\"\"\"\n",
    "\n",
    "#             plasticity = Plasticity()\n",
    "\n",
    "#             STDP \n",
    "#             Wee_t = plasticity.stdp(Wee[i],x_buffer,cutoff_weights = (0.0,1.0))\n",
    "              \n",
    "#             Intrinsic plasticity\n",
    "#             Te_t = plasticity.ip(Te[i],x_buffer)\n",
    "              \n",
    "#             Structural plasticity\n",
    "#             Wee_t = plasticity.structural_plasticity(Wee_t)      \n",
    "            \n",
    "#             iSTDP \n",
    "#             Wei_t = plasticity.istdp(Wei[i],x_buffer,y_buffer,cutoff_weights = (0.0,1.0))\n",
    "            \n",
    "#             Synaptic scaling Wee\n",
    "#             Wee_t = Plasticity().ss(Wee_t)\n",
    "            \n",
    "#             Synaptic scaling Wei\n",
    "#             Wei_t = Plasticity().ss(Wei_t)\n",
    "\n",
    "            \"\"\"Assign the matrices to the matrix collections\"\"\"\n",
    "            matrix_collection.weight_matrix(Wee[i], Wei[i], Wie[i], i)\n",
    "            matrix_collection.threshold_matrix(Te[i], Ti[i], i)\n",
    "            matrix_collection.network_activity_t(x_buffer, y_buffer, i)\n",
    "            \n",
    "            X_all[i] = x_buffer[:,1]\n",
    "            Y_all[i] = y_buffer[:,1]\n",
    "            R_all[i] = r\n",
    "   \n",
    "        plastic_matrices = {'Wee':matrix_collection.Wee[-1], \n",
    "                            'Wei': matrix_collection.Wei[-1], \n",
    "                            'Wie':matrix_collection.Wie[-1],\n",
    "                            'Te': matrix_collection.Te[-1], 'Ti': matrix_collection.Ti[-1],\n",
    "                            'X': X[-1], 'Y': Y[-1]}\n",
    "        \n",
    "        return plastic_matrices,X_all,Y_all,R_all,frac_pos_active_conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PINGPONG AGENT ENVIRONEMNT ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Architecture\n",
    "\n",
    "# Take in inputs from the screen and preprocess them\n",
    "# Pass them into SORN (200) with 10 input units in the reservoir\n",
    "# Update the weights of the SORN input layer and the output layer using gradient descent\n",
    "# weights['1'] - Matrix that holds weights of pixels passing into SORN reservoir. Dimensions: [10 x 80 x 80] -> [10 x 6400]\n",
    "# weights['2'] - Matrix that holds weights of SORN reservoir passing into output. Dimensions: [1 x 200]\n",
    "\n",
    "# Process is:\n",
    "\n",
    "# processed_observations = image vector - [6400 x 1] array\n",
    "# Compute input_layer_values = weights['1'] dot processed_observations ([10 x 6400] dot [6400 x 1]) -> [10 x 1] - this gives initial activation values.\n",
    "# Next we need to transform those either via a sigmoid or an ReLU of some sort. Let's use ReLU\n",
    "# ReLU(input_layer_values)\n",
    "# Next pass this values into SORN input units\n",
    "# Next we need to pass the reservoir activity of SORN one layer further\n",
    "# output_layer_value = weights['2'] dot hidden_layer_values ([1 x 200] dot [200 x 1] -> [1 x 1])\n",
    "# Now our output layer is the probability of going up or down. Let's make sure this output is between 0 and 1 by passing it through a sigmoid\n",
    "# p = sigmoid(output_layer_value)\n",
    "\n",
    "# Learning after round has finished:\n",
    "\n",
    "# Figure out the result\n",
    "# Compute the error\n",
    "# Use the error to calculate the gradient\n",
    "    # The below dimensions all assume we had exactly 10 frames in the round (not necessarily true!)\n",
    "    # dC_dw2 = hidden_layer_values^T dot gradient_log_p ([1 x 2000] dot [2000 x 1] -> 1x1)\n",
    "    # delta_1 = gradient_log_p outer_product weights['2'] = [2000 x 1] outer_product [1 x 200] ([2000 x 200])\n",
    "    # dC_dw1 = delta_1^T dot input_observations ([10 x 100]x dot [100 x 64000] -> [10 x 64000])\n",
    "\n",
    "# After some batch size of rounds has finished,\n",
    "    # Use rmsprop to move weights['1'] and weights['2'] in the direction of the gradient\n",
    "# Repeat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Init sorn object\n",
    "    \n",
    "train_sorn = TrainSorn()\n",
    "train_sorn_plasticity = TrainSornPlasticity()\n",
    "    \n",
    "def downsample(image):\n",
    "    # Take only alternate pixels - basically halves the resolution of the image (which is fine for us)\n",
    "    return image[::2, ::2, :]\n",
    "\n",
    "def remove_color(image):\n",
    "    \"\"\"Convert all color (RGB is the third dimension in the image)\"\"\"\n",
    "    return image[:, :, 0]\n",
    "\n",
    "def remove_background(image):\n",
    "    image[image == 144] = 0\n",
    "    image[image == 109] = 0\n",
    "    return image\n",
    "\n",
    "def preprocess_observations(input_observation, prev_processed_observation, input_dimensions):\n",
    "    \"\"\" convert the 210x160x3 uint8 frame into a 6400 float vector \"\"\"\n",
    "    processed_observation = input_observation[35:195] # crop\n",
    "    processed_observation = downsample(processed_observation)\n",
    "    processed_observation = remove_color(processed_observation)\n",
    "    processed_observation = remove_background(processed_observation)\n",
    "    processed_observation[processed_observation != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    # Convert from 80 x 80 matrix to 1600 x 1 matrix\n",
    "    processed_observation = processed_observation.astype(np.float).ravel()\n",
    "\n",
    "    # subtract the previous frame from the current one so we are only processing on changes in the game\n",
    "    if prev_processed_observation is not None:\n",
    "        input_observation = processed_observation - prev_processed_observation\n",
    "    else:\n",
    "        input_observation = np.zeros(input_dimensions)\n",
    "    # store the previous frame so we can subtract from it next time\n",
    "    prev_processed_observations = processed_observation\n",
    "    return input_observation, prev_processed_observations\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def relu(vector):\n",
    "    vector[vector < 0] = 0\n",
    "    return vector\n",
    "\n",
    "def apply_neural_nets(observation_matrix, weights,plastic_matrices,episode):\n",
    "    \n",
    "    \"\"\" Based on the observation_matrix and weights, compute the new hidden layer values and the new output layer values\"\"\"\n",
    "    input_layer_values = np.dot(weights['1'], observation_matrix)\n",
    "    input_layer_values = relu(input_layer_values)\n",
    "    \n",
    "    \n",
    "    if episode < 1000:\n",
    "        \n",
    "        # Note that reservoir states replaces variable name X_all\n",
    "        plastic_matrices,reservoir_states,Y_all,R_all,frac_pos_active_conn = train_sorn_plasticity.train_sorn(phase = 'Plasticity',\n",
    "                                                                                       matrices = plastic_matrices,\n",
    "                                                                                       inputs = input_layer_values)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        if episode == 1000:\n",
    "            with open('plasticmatrices.pkl','rb') as f:  \n",
    "                plastic_matrices,X_all,Y_all,R_all,frac_pos_active_conn = pickle.load(f)\n",
    "        \n",
    "        # Note that reservoir states replaces variable name X_all\n",
    "        plastic_matrices,reservoir_states,Y_all,R_all,frac_pos_active_conn = train_sorn.train_sorn(phase = 'Training',\n",
    "                                                                                       matrices = plastic_matrices,\n",
    "                                                                                       inputs = input_layer_values)\n",
    "        \n",
    "    output_layer_values = np.dot(reservoir_states, weights['2'])\n",
    "    output_layer_values = sigmoid(output_layer_values)\n",
    "    return input_layer_values, output_layer_values, plastic_matrices\n",
    "\n",
    "def choose_action(probability):\n",
    "    random_value = np.random.uniform()\n",
    "    if random_value < probability:\n",
    "        # signifies up in openai gym\n",
    "        return 2\n",
    "    else:\n",
    "         # signifies down in openai gym\n",
    "        return 3\n",
    "\n",
    "def compute_gradient(gradient_log_p, input_layer_values, observation_values, weights):\n",
    "    \n",
    "    delta_L = gradient_log_p\n",
    "    dC_dw2 = np.dot(input_layer_values.T, delta_L).ravel()\n",
    "    delta_l2 = np.outer(delta_L, weights['2'])\n",
    "    delta_l2 = relu(delta_l2)\n",
    "    dC_dw1 = np.dot(delta_l2.T, observation_values)\n",
    "    return {\n",
    "        '1': dC_dw1,\n",
    "        '2': dC_dw2\n",
    "    }\n",
    "\n",
    "def update_weights(episode,weights, expectation_g_squared, g_dict, decay_rate, learning_rate):\n",
    "    \n",
    "    epsilon = 1e-4\n",
    "    for layer_name in weights.keys():\n",
    "        g = g_dict[layer_name]\n",
    "        expectation_g_squared[layer_name] = decay_rate * expectation_g_squared[layer_name] + (1 - decay_rate) * g**2\n",
    "        weights[layer_name] += (learning_rate * g)/(np.sqrt(expectation_g_squared[layer_name] + epsilon))\n",
    "        g_dict[layer_name] = np.zeros_like(weights[layer_name]) # reset batch gradient buffer\n",
    "    \n",
    "    if episode % 200 == 0:\n",
    "        \n",
    "        with open('%s.pickle' %episode, 'wb') as f:\n",
    "            pickle.dump(weights, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        \n",
    "def discount_rewards(rewards, gamma):\n",
    "    \"\"\" Actions you took 20 steps before the end result are less important to the overall result than an action you took a step ago.\n",
    "    This implements that logic by discounting the reward on previous actions based on how long ago they were taken\"\"\"\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, rewards.size)):\n",
    "        if rewards[t] != 0:\n",
    "            running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + rewards[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_with_rewards(gradient_log_p, episode_rewards, gamma):\n",
    "    \"\"\" discount the gradient with the normalized rewards \"\"\"\n",
    "    discounted_episode_rewards = discount_rewards(episode_rewards, gamma)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_episode_rewards -= np.mean(discounted_episode_rewards)\n",
    "    discounted_episode_rewards /= np.std(discounted_episode_rewards)\n",
    "    return gradient_log_p * discounted_episode_rewards\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"Pong-v0\")\n",
    "    # env = gym.make(\"CartPole-v0\")\n",
    "    observation = env.reset() # This gets us the image\n",
    "    \n",
    "    env.seed(10)\n",
    "    \n",
    "    # hyperparameters\n",
    "    episode_number = 0\n",
    "    batch_size = 5\n",
    "    gamma = 0.99 # discount factor for reward\n",
    "    decay_rate = 0.99\n",
    "    num_hidden_layer_neurons = 200\n",
    "    input_dimensions = 80 * 80\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    episode_number = 0\n",
    "    reward_sum = 0\n",
    "    running_reward = None\n",
    "    prev_processed_observations = None\n",
    "\n",
    "    weights = {\n",
    "        '1': np.random.randn(num_hidden_layer_neurons, input_dimensions) / np.sqrt(input_dimensions),\n",
    "        '2': np.random.randn(num_hidden_layer_neurons) / np.sqrt(num_hidden_layer_neurons)\n",
    "    }\n",
    "\n",
    "    # To be used with rmsprop algorithm (http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop)\n",
    "    expectation_g_squared = {}\n",
    "    g_dict = {}\n",
    "    for layer_name in weights.keys():\n",
    "        expectation_g_squared[layer_name] = np.zeros_like(weights[layer_name])\n",
    "        g_dict[layer_name] = np.zeros_like(weights[layer_name])\n",
    "\n",
    "    episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], []\n",
    "\n",
    "    with open('stdp2013_3020k.pkl','rb') as f:  \n",
    "        plastic_matrices,X_all,Y_all,R_all,frac_pos_active_conn = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        # env.render()\n",
    "        processed_observations, prev_processed_observations = preprocess_observations(observation, prev_processed_observations, input_dimensions)\n",
    "        \n",
    "        \n",
    "        hidden_layer_values, up_probability,plastic_matrices = apply_neural_nets(processed_observations, weights, plastic_matrices,episode_number)\n",
    "    \n",
    "        episode_observations.append(processed_observations)\n",
    "        episode_hidden_layer_values.append(hidden_layer_values)\n",
    "\n",
    "        action = choose_action(up_probability)\n",
    "\n",
    "        # carry out the chosen action\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        reward_sum += reward\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        # see here: http://cs231n.github.io/neural-networks-2/#losses\n",
    "        fake_label = 1 if action == 2 else 0\n",
    "        loss_function_gradient = fake_label - up_probability\n",
    "        episode_gradient_log_ps.append(loss_function_gradient)\n",
    "\n",
    "\n",
    "        if done: # an episode finished\n",
    "            episode_number += 1\n",
    "            print(episode_number)\n",
    "            # Combine the following values for the episode\n",
    "            episode_hidden_layer_values = np.vstack(episode_hidden_layer_values)\n",
    "            episode_observations = np.vstack(episode_observations)\n",
    "            episode_gradient_log_ps = np.vstack(episode_gradient_log_ps)\n",
    "            episode_rewards = np.vstack(episode_rewards)\n",
    "\n",
    "            # Tweak the gradient of the log_ps based on the discounted rewards\n",
    "            episode_gradient_log_ps_discounted = discount_with_rewards(episode_gradient_log_ps, episode_rewards, gamma)\n",
    "\n",
    "            gradient = compute_gradient(\n",
    "              episode_gradient_log_ps_discounted,\n",
    "              episode_hidden_layer_values,\n",
    "              episode_observations,\n",
    "              weights\n",
    "            )\n",
    "\n",
    "            # Sum the gradient for use when we hit the batch size\n",
    "            for layer_name in gradient:\n",
    "                g_dict[layer_name] += gradient[layer_name]\n",
    "\n",
    "            if episode_number % batch_size == 0:\n",
    "                update_weights(episode_number,weights, expectation_g_squared, g_dict, decay_rate, learning_rate)\n",
    "\n",
    "            episode_hidden_layer_values, episode_observations, episode_gradient_log_ps, episode_rewards = [], [], [], [] # reset values\n",
    "            observation = env.reset() # reset env\n",
    "            running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "            print ('resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward))\n",
    "            reward_sum = 0\n",
    "            prev_processed_observations = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "resetting env. episode reward total was -21.000000. running mean: -21.000000\n",
      "2\n",
      "resetting env. episode reward total was -21.000000. running mean: -21.000000\n",
      "3\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.990000\n",
      "4\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.990100\n",
      "5\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.990199\n",
      "6\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.980297\n",
      "7\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.980494\n",
      "8\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.980689\n",
      "9\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.970882\n",
      "10\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.961173\n",
      "11\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.951562\n",
      "12\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.952046\n",
      "13\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.952526\n",
      "14\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.953000\n",
      "15\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.943470\n",
      "16\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.944036\n",
      "17\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.934595\n",
      "18\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.935249\n",
      "19\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.935897\n",
      "20\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.926538\n",
      "21\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.917272\n",
      "22\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.918100\n",
      "23\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.918919\n",
      "24\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.909730\n",
      "25\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.910632\n",
      "26\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.911526\n",
      "27\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.902411\n",
      "28\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.903387\n",
      "29\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.894353\n",
      "30\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.885409\n",
      "31\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.886555\n",
      "32\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.887690\n",
      "33\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.878813\n",
      "34\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.870025\n",
      "35\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.871324\n",
      "36\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.872611\n",
      "37\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.873885\n",
      "38\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.875146\n",
      "39\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.876395\n",
      "40\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.867631\n",
      "41\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.868954\n",
      "42\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.870265\n",
      "43\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.871562\n",
      "44\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.862847\n",
      "45\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.864218\n",
      "46\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.855576\n",
      "47\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.857020\n",
      "48\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.838450\n",
      "49\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.830065\n",
      "50\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.831765\n",
      "51\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.833447\n",
      "52\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.835113\n",
      "53\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.826762\n",
      "54\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.828494\n",
      "55\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.830209\n",
      "56\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.831907\n",
      "57\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.833588\n",
      "58\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.815252\n",
      "59\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.797099\n",
      "60\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.799128\n",
      "61\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.791137\n",
      "62\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.773226\n",
      "63\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.775494\n",
      "64\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.767739\n",
      "65\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.770061\n",
      "66\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.752361\n",
      "67\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.734837\n",
      "68\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.737489\n",
      "69\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.740114\n",
      "70\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.742713\n",
      "71\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.735285\n",
      "72\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.727933\n",
      "73\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.730653\n",
      "74\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.713347\n",
      "75\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.706213\n",
      "76\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.699151\n",
      "77\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.692160\n",
      "78\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.685238\n",
      "79\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.688386\n",
      "80\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.691502\n",
      "81\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.684587\n",
      "82\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.687741\n",
      "83\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.690864\n",
      "84\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.683955\n",
      "85\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.687115\n",
      "86\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.690244\n",
      "87\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.693342\n",
      "88\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.686408\n",
      "89\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.679544\n",
      "90\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.672749\n",
      "91\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.676021\n",
      "92\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.679261\n",
      "93\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.672468\n",
      "94\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.675744\n",
      "95\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.678986\n",
      "96\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.682196\n",
      "97\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.685375\n",
      "98\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.688521\n",
      "99\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.681636\n",
      "100\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.664819\n",
      "101\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.668171\n",
      "102\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.651489\n",
      "103\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.644974\n",
      "104\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.638525\n",
      "105\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.642139\n",
      "106\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.645718\n",
      "107\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.629261\n",
      "108\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.632968\n",
      "109\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.636639\n",
      "110\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.630272\n",
      "111\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.623969\n",
      "112\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.617730\n",
      "113\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.601552\n",
      "114\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.605537\n",
      "115\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.599482\n",
      "116\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.593487\n",
      "117\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.597552\n",
      "118\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.601576\n",
      "119\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.605561\n",
      "120\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.609505\n",
      "121\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.613410\n",
      "122\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.607276\n",
      "123\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.601203\n",
      "124\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.595191\n",
      "125\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.599239\n",
      "126\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.603247\n",
      "127\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.597214\n",
      "128\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.591242\n",
      "129\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.595330\n",
      "130\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.589376\n",
      "131\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.583483\n",
      "132\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.567648\n",
      "133\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.551971\n",
      "134\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.556452\n",
      "135\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.560887\n",
      "136\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.565278\n",
      "137\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.569625\n",
      "138\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.573929\n",
      "139\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.578190\n",
      "140\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.582408\n",
      "141\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.586584\n",
      "142\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.590718\n",
      "143\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.594811\n",
      "144\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.598863\n",
      "145\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.602874\n",
      "146\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.596845\n",
      "147\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.590877\n",
      "148\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.594968\n",
      "149\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.599019\n",
      "150\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.603028\n",
      "151\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.596998\n",
      "152\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.591028\n",
      "153\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.585118\n",
      "154\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.569267\n",
      "155\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.573574\n",
      "156\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.577838\n",
      "157\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.562060\n",
      "158\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.556439\n",
      "159\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.550875\n",
      "160\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.545366\n",
      "161\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.539912\n",
      "162\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.544513\n",
      "163\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.549068\n",
      "164\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.543578\n",
      "165\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.548142\n",
      "166\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.552660\n",
      "167\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.557134\n",
      "168\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.551562\n",
      "169\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.556047\n",
      "170\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.550486\n",
      "171\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.554981\n",
      "172\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.559432\n",
      "173\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.553837\n",
      "174\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.558299\n",
      "175\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.562716\n",
      "176\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.567089\n",
      "177\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.571418\n",
      "178\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.565704\n",
      "179\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.570047\n",
      "180\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.564346\n",
      "181\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.558703\n",
      "182\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.563116\n",
      "183\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.567485\n",
      "184\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.571810\n",
      "185\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.576092\n",
      "186\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.580331\n",
      "187\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.584527\n",
      "188\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.578682\n",
      "189\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.582895\n",
      "190\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.587066\n",
      "191\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.591196\n",
      "192\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.585284\n",
      "193\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.589431\n",
      "194\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.583537\n",
      "195\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.587701\n",
      "196\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.591824\n",
      "197\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.585906\n",
      "198\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.580047\n",
      "199\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.544246\n",
      "200\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.548804\n",
      "201\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.553316\n",
      "202\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.537783\n",
      "203\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.532405\n",
      "204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was -21.000000. running mean: -20.537081\n",
      "205\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.541710\n",
      "206\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.536293\n",
      "207\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.530930\n",
      "208\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.505621\n",
      "209\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.500565\n",
      "210\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.495559\n",
      "211\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.500603\n",
      "212\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.505597\n",
      "213\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.490541\n",
      "214\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.485636\n",
      "215\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.490780\n",
      "216\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.495872\n",
      "217\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.490913\n",
      "218\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.496004\n",
      "219\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.471044\n",
      "220\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.466333\n",
      "221\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.471670\n",
      "222\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.476953\n",
      "223\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.482184\n",
      "224\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.487362\n",
      "225\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.472488\n",
      "226\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.477763\n",
      "227\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.472986\n",
      "228\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.468256\n",
      "229\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.473573\n",
      "230\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.478838\n",
      "231\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.484049\n",
      "232\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.489209\n",
      "233\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.494317\n",
      "234\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.489374\n",
      "235\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.494480\n",
      "236\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.489535\n",
      "237\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.494640\n",
      "238\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.489693\n",
      "239\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.484796\n",
      "240\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.479948\n",
      "241\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.455149\n",
      "242\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.440597\n",
      "243\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.446191\n",
      "244\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.451730\n",
      "245\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.447212\n",
      "246\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.452740\n",
      "247\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.458213\n",
      "248\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.433631\n",
      "249\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.429294\n",
      "250\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.395001\n",
      "251\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.401051\n",
      "252\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.407041\n",
      "253\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.392970\n",
      "254\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.399041\n",
      "255\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.405050\n",
      "256\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.401000\n",
      "257\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.406990\n",
      "258\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.392920\n",
      "259\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.398991\n",
      "260\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.395001\n",
      "261\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.391051\n",
      "262\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.397140\n",
      "263\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.403169\n",
      "264\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.399137\n",
      "265\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.395146\n",
      "266\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.401194\n",
      "267\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.407182\n",
      "268\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.393111\n",
      "269\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.399179\n",
      "270\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.405188\n",
      "271\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.401136\n",
      "272\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.387124\n",
      "273\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.393253\n",
      "274\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.399321\n",
      "275\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.395327\n",
      "276\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.381374\n",
      "277\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.387560\n",
      "278\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.393685\n",
      "279\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.399748\n",
      "280\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.405751\n",
      "281\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.411693\n",
      "282\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.417576\n",
      "283\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.413400\n",
      "284\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.419266\n",
      "285\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.425074\n",
      "286\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.430823\n",
      "287\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.436515\n",
      "288\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.442150\n",
      "289\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.447728\n",
      "290\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.453251\n",
      "291\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.458718\n",
      "292\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.464131\n",
      "293\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.469490\n",
      "294\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.474795\n",
      "295\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.480047\n",
      "296\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.475246\n",
      "297\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.480494\n",
      "298\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.485689\n",
      "299\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.490832\n",
      "300\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.485924\n",
      "301\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.491065\n",
      "302\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.486154\n",
      "303\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.491292\n",
      "304\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.496379\n",
      "305\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.501416\n",
      "306\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.506402\n",
      "307\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.501338\n",
      "308\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.506324\n",
      "309\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.511261\n",
      "310\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.506148\n",
      "311\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.501087\n",
      "312\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.506076\n",
      "313\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.511015\n",
      "314\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.495905\n",
      "315\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.490946\n",
      "316\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.486037\n",
      "317\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.481176\n",
      "318\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.486364\n",
      "319\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.491501\n",
      "320\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.486586\n",
      "321\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.491720\n",
      "322\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.496803\n",
      "323\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.491835\n",
      "324\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.486916\n",
      "325\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.492047\n",
      "326\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.497127\n",
      "327\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.502155\n",
      "328\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.507134\n",
      "329\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.512063\n",
      "330\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.516942\n",
      "331\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.521772\n",
      "332\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.526555\n",
      "333\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.531289\n",
      "334\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.535976\n",
      "335\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.540617\n",
      "336\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.535210\n",
      "337\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.539858\n",
      "338\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.544460\n",
      "339\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.549015\n",
      "340\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.543525\n",
      "341\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.548090\n",
      "342\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.552609\n",
      "343\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.557083\n",
      "344\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.561512\n",
      "345\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.555897\n",
      "346\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.540338\n",
      "347\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.544934\n",
      "348\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.549485\n",
      "349\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.523990\n",
      "350\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.528750\n",
      "351\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.533463\n",
      "352\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.528128\n",
      "353\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.532847\n",
      "354\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.537518\n",
      "355\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.532143\n",
      "356\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.536822\n",
      "357\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.541454\n",
      "358\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.516039\n",
      "359\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.510879\n",
      "360\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.515770\n",
      "361\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.520612\n",
      "362\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.505406\n",
      "363\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.510352\n",
      "364\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.505248\n",
      "365\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.500196\n",
      "366\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.505194\n",
      "367\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.510142\n",
      "368\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.515041\n",
      "369\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.519890\n",
      "370\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.514691\n",
      "371\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.519544\n",
      "372\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.524349\n",
      "373\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.529106\n",
      "374\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.533814\n",
      "375\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.538476\n",
      "376\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.543092\n",
      "377\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.527661\n",
      "378\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.532384\n",
      "379\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.497060\n",
      "380\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.492090\n",
      "381\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.497169\n",
      "382\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.502197\n",
      "383\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.507175\n",
      "384\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.512103\n",
      "385\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.506982\n",
      "386\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.511912\n",
      "387\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.516793\n",
      "388\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.521625\n",
      "389\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.526409\n",
      "390\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.521145\n",
      "391\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.515934\n",
      "392\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.500774\n",
      "393\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.505767\n",
      "394\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.500709\n",
      "395\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.495702\n",
      "396\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.500745\n",
      "397\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.505737\n",
      "398\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.510680\n",
      "399\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.515573\n",
      "400\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.510417\n",
      "401\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.495313\n",
      "402\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.500360\n",
      "403\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.485356\n",
      "404\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.490503\n",
      "405\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.495598\n",
      "406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was -20.000000. running mean: -20.490642\n",
      "407\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.495735\n",
      "408\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.480778\n",
      "409\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.485970\n",
      "410\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.491111\n",
      "411\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.496200\n",
      "412\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.491238\n",
      "413\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.496325\n",
      "414\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.491362\n",
      "415\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.496448\n",
      "416\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.481484\n",
      "417\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.486669\n",
      "418\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.491802\n",
      "419\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.496884\n",
      "420\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.491915\n",
      "421\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.496996\n",
      "422\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.502026\n",
      "423\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.507006\n",
      "424\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.511936\n",
      "425\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.496817\n",
      "426\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.491848\n",
      "427\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.496930\n",
      "428\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.501961\n",
      "429\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.476941\n",
      "430\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.482172\n",
      "431\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.477350\n",
      "432\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.482576\n",
      "433\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.457751\n",
      "434\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.463173\n",
      "435\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.458541\n",
      "436\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.463956\n",
      "437\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.459316\n",
      "438\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.454723\n",
      "439\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.450176\n",
      "440\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.455674\n",
      "441\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.461118\n",
      "442\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.456506\n",
      "443\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.461941\n",
      "444\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.457322\n",
      "445\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.462749\n",
      "446\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.468121\n",
      "447\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.463440\n",
      "448\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.468806\n",
      "449\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.474118\n",
      "450\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.479376\n",
      "451\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.474583\n",
      "452\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.479837\n",
      "453\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.485038\n",
      "454\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.490188\n",
      "455\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.485286\n",
      "456\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.490433\n",
      "457\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.485529\n",
      "458\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.490674\n",
      "459\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.485767\n",
      "460\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.490909\n",
      "461\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.486000\n",
      "462\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.491140\n",
      "463\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.496229\n",
      "464\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.491266\n",
      "465\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.496354\n",
      "466\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.501390\n",
      "467\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.506376\n",
      "468\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.511313\n",
      "469\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.516199\n",
      "470\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.511037\n",
      "471\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.515927\n",
      "472\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.510768\n",
      "473\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.515660\n",
      "474\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.520504\n",
      "475\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.525299\n",
      "476\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.530046\n",
      "477\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.534745\n",
      "478\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.539398\n",
      "479\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.534004\n",
      "480\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.528664\n",
      "481\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.533377\n",
      "482\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.538043\n",
      "483\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.542663\n",
      "484\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.537236\n",
      "485\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.531864\n",
      "486\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.536545\n",
      "487\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.541180\n",
      "488\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.535768\n",
      "489\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.540410\n",
      "490\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.545006\n",
      "491\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.549556\n",
      "492\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.554060\n",
      "493\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.558520\n",
      "494\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.542935\n",
      "495\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.537505\n",
      "496\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.542130\n",
      "497\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.546709\n",
      "498\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.551242\n",
      "499\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.545729\n",
      "500\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.540272\n",
      "501\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.534869\n",
      "502\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.539521\n",
      "503\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.544126\n",
      "504\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.548684\n",
      "505\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.553197\n",
      "506\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.537665\n",
      "507\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.532289\n",
      "508\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.526966\n",
      "509\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.511696\n",
      "510\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.506579\n",
      "511\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.511514\n",
      "512\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.496398\n",
      "513\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.491434\n",
      "514\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.496520\n",
      "515\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.491555\n",
      "516\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.476639\n",
      "517\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.451873\n",
      "518\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.457354\n",
      "519\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.462781\n",
      "520\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.448153\n",
      "521\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.453671\n",
      "522\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.439135\n",
      "523\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.434743\n",
      "524\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.440396\n",
      "525\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.445992\n",
      "526\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.451532\n",
      "527\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.447017\n",
      "528\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.442546\n",
      "529\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.428121\n",
      "530\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.433840\n",
      "531\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.419501\n",
      "532\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.385306\n",
      "533\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.381453\n",
      "534\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.367639\n",
      "535\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.373962\n",
      "536\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.370223\n",
      "537\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.366521\n",
      "538\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.352855\n",
      "539\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.359327\n",
      "540\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.355734\n",
      "541\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.352176\n",
      "542\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.348654\n",
      "543\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.335168\n",
      "544\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.331816\n",
      "545\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.328498\n",
      "546\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.335213\n",
      "547\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.341861\n",
      "548\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.338442\n",
      "549\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.325058\n",
      "550\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.331807\n",
      "551\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.338489\n",
      "552\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.345104\n",
      "553\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.311653\n",
      "554\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.318537\n",
      "555\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.325351\n",
      "556\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.332098\n",
      "557\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.338777\n",
      "558\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.335389\n",
      "559\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.342035\n",
      "560\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.338615\n",
      "561\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.325229\n",
      "562\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.331976\n",
      "563\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.328657\n",
      "564\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.315370\n",
      "565\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.322216\n",
      "566\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.328994\n",
      "567\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.335704\n",
      "568\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.332347\n",
      "569\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.329024\n",
      "570\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.335734\n",
      "571\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.342376\n",
      "572\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.338952\n",
      "573\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.345563\n",
      "574\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.352107\n",
      "575\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.348586\n",
      "576\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.345100\n",
      "577\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.341649\n",
      "578\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.338233\n",
      "579\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.334851\n",
      "580\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.321502\n",
      "581\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.318287\n",
      "582\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.325104\n",
      "583\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.331853\n",
      "584\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.328535\n",
      "585\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.335249\n",
      "586\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.321897\n",
      "587\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.328678\n",
      "588\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.315391\n",
      "589\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.302237\n",
      "590\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.299215\n",
      "591\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.296223\n",
      "592\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.303260\n",
      "593\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.280228\n",
      "594\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.277425\n",
      "595\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.284651\n",
      "596\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.281805\n",
      "597\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.268987\n",
      "598\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.276297\n",
      "599\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.273534\n",
      "600\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.280798\n",
      "601\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.277990\n",
      "602\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.285211\n",
      "603\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.292358\n",
      "604\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.299435\n",
      "605\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.276441\n",
      "606\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.283676\n",
      "607\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.290839\n",
      "608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was -20.000000. running mean: -20.287931\n",
      "609\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.285052\n",
      "610\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.282201\n",
      "611\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.279379\n",
      "612\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.286585\n",
      "613\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.293720\n",
      "614\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.280782\n",
      "615\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.287974\n",
      "616\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.265095\n",
      "617\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.262444\n",
      "618\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.259819\n",
      "619\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.267221\n",
      "620\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.264549\n",
      "621\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.271903\n",
      "622\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.269184\n",
      "623\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.276493\n",
      "624\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.263728\n",
      "625\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.261090\n",
      "626\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.268479\n",
      "627\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.265795\n",
      "628\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.273137\n",
      "629\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.270405\n",
      "630\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.237701\n",
      "631\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.245324\n",
      "632\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.242871\n",
      "633\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.240442\n",
      "634\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.248038\n",
      "635\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.255558\n",
      "636\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.253002\n",
      "637\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.250472\n",
      "638\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.247967\n",
      "639\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.255488\n",
      "640\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.262933\n",
      "641\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.270303\n",
      "642\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.257600\n",
      "643\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.245024\n",
      "644\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.252574\n",
      "645\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.250048\n",
      "646\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.257548\n",
      "647\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.254972\n",
      "648\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.252423\n",
      "649\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.249898\n",
      "650\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.247399\n",
      "651\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.244925\n",
      "652\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.242476\n",
      "653\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.250051\n",
      "654\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.247551\n",
      "655\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.215075\n",
      "656\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.222925\n",
      "657\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.230695\n",
      "658\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.238388\n",
      "659\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.246005\n",
      "660\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.253545\n",
      "661\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.251009\n",
      "662\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.248499\n",
      "663\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.256014\n",
      "664\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.263454\n",
      "665\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.270819\n",
      "666\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.278111\n",
      "667\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.285330\n",
      "668\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.272477\n",
      "669\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.269752\n",
      "670\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.277054\n",
      "671\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.284284\n",
      "672\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.271441\n",
      "673\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.278727\n",
      "674\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.285939\n",
      "675\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.293080\n",
      "676\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.280149\n",
      "677\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.267348\n",
      "678\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.274674\n",
      "679\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.281927\n",
      "680\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.289108\n",
      "681\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.286217\n",
      "682\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.293355\n",
      "683\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.300421\n",
      "684\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.307417\n",
      "685\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.314343\n",
      "686\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.321200\n",
      "687\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.327988\n",
      "688\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.334708\n",
      "689\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.321361\n",
      "690\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.328147\n",
      "691\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.334866\n",
      "692\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.341517\n",
      "693\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.318102\n",
      "694\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.314921\n",
      "695\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.321771\n",
      "696\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.318554\n",
      "697\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.325368\n",
      "698\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.332115\n",
      "699\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.328793\n",
      "700\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.325505\n",
      "701\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.332250\n",
      "702\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.338928\n",
      "703\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.345539\n",
      "704\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.352083\n",
      "705\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.358562\n",
      "706\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.344977\n",
      "707\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.341527\n",
      "708\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.348112\n",
      "709\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.354631\n",
      "710\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.341084\n",
      "711\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.347673\n",
      "712\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.334197\n",
      "713\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.320855\n",
      "714\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.327646\n",
      "715\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.324370\n",
      "716\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.331126\n",
      "717\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.327815\n",
      "718\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.324537\n",
      "719\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.331291\n",
      "720\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.337978\n",
      "721\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.334599\n",
      "722\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.331253\n",
      "723\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.327940\n",
      "724\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.314661\n",
      "725\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.311514\n",
      "726\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.318399\n",
      "727\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.305215\n",
      "728\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.302163\n",
      "729\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.309141\n",
      "730\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.306050\n",
      "731\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.302989\n",
      "732\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.289959\n",
      "733\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.287060\n",
      "734\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.294189\n",
      "735\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.301247\n",
      "736\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.278235\n",
      "737\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.285452\n",
      "738\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.282598\n",
      "739\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.289772\n",
      "740\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.296874\n",
      "741\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.293906\n",
      "742\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.300966\n",
      "743\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.307957\n",
      "744\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.314877\n",
      "745\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.321728\n",
      "746\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.318511\n",
      "747\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.325326\n",
      "748\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.312073\n",
      "749\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.318952\n",
      "750\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.305763\n",
      "751\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.312705\n",
      "752\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.309578\n",
      "753\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.296482\n",
      "754\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.283517\n",
      "755\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.290682\n",
      "756\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.277775\n",
      "757\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.264998\n",
      "758\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.262348\n",
      "759\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.249724\n",
      "760\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.257227\n",
      "761\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.244655\n",
      "762\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.232208\n",
      "763\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.219886\n",
      "764\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.227687\n",
      "765\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.225410\n",
      "766\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.223156\n",
      "767\n",
      "resetting env. episode reward total was -16.000000. running mean: -20.180925\n",
      "768\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.189115\n",
      "769\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.187224\n",
      "770\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.185352\n",
      "771\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.163498\n",
      "772\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.161863\n",
      "773\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.170245\n",
      "774\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.158542\n",
      "775\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.156957\n",
      "776\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.135387\n",
      "777\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.144033\n",
      "778\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.132593\n",
      "779\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.121267\n",
      "780\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.110055\n",
      "781\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.078954\n",
      "782\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.078164\n",
      "783\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.077383\n",
      "784\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.076609\n",
      "785\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.065843\n",
      "786\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.055184\n",
      "787\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.064633\n",
      "788\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.073986\n",
      "789\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.073246\n",
      "790\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.072514\n",
      "791\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.081789\n",
      "792\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.080971\n",
      "793\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.090161\n",
      "794\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.099260\n",
      "795\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.098267\n",
      "796\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.077284\n",
      "797\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.086511\n",
      "798\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.095646\n",
      "799\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.104690\n",
      "800\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.113643\n",
      "801\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.112507\n",
      "802\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.121382\n",
      "803\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.110168\n",
      "804\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.119066\n",
      "805\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.117875\n",
      "806\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.106697\n",
      "807\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.105630\n",
      "808\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.104573\n",
      "809\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.093528\n",
      "810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was -20.000000. running mean: -20.092592\n",
      "811\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.091666\n",
      "812\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.090750\n",
      "813\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.089842\n",
      "814\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.088944\n",
      "815\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.088054\n",
      "816\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.087174\n",
      "817\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.076302\n",
      "818\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.085539\n",
      "819\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.094684\n",
      "820\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.063737\n",
      "821\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.073099\n",
      "822\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.082368\n",
      "823\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.081545\n",
      "824\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.070729\n",
      "825\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.040022\n",
      "826\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.039622\n",
      "827\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.039226\n",
      "828\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.048833\n",
      "829\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.058345\n",
      "830\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.067762\n",
      "831\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.077084\n",
      "832\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.086313\n",
      "833\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.095450\n",
      "834\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.094496\n",
      "835\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.103551\n",
      "836\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.112515\n",
      "837\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.121390\n",
      "838\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.120176\n",
      "839\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.118974\n",
      "840\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.107784\n",
      "841\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.116707\n",
      "842\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.125540\n",
      "843\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.134284\n",
      "844\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.142941\n",
      "845\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.141512\n",
      "846\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.150097\n",
      "847\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.158596\n",
      "848\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.167010\n",
      "849\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.155340\n",
      "850\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.153786\n",
      "851\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.152249\n",
      "852\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.160726\n",
      "853\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.149119\n",
      "854\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.147628\n",
      "855\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.136151\n",
      "856\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.124790\n",
      "857\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.123542\n",
      "858\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.112306\n",
      "859\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.111183\n",
      "860\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.100072\n",
      "861\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.109071\n",
      "862\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.107980\n",
      "863\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.116900\n",
      "864\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.115731\n",
      "865\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.104574\n",
      "866\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.113528\n",
      "867\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.122393\n",
      "868\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.121169\n",
      "869\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.129957\n",
      "870\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.118658\n",
      "871\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.097471\n",
      "872\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.106497\n",
      "873\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.115432\n",
      "874\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.094277\n",
      "875\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.103334\n",
      "876\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.112301\n",
      "877\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.121178\n",
      "878\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.099966\n",
      "879\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.098967\n",
      "880\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.107977\n",
      "881\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.096897\n",
      "882\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.095928\n",
      "883\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.084969\n",
      "884\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.074119\n",
      "885\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.083378\n",
      "886\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.092544\n",
      "887\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.081619\n",
      "888\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.070803\n",
      "889\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.070095\n",
      "890\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.079394\n",
      "891\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.088600\n",
      "892\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.097714\n",
      "893\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.106737\n",
      "894\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.115669\n",
      "895\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.114513\n",
      "896\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.093367\n",
      "897\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.102434\n",
      "898\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.111409\n",
      "899\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.120295\n",
      "900\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.119092\n",
      "901\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.117901\n",
      "902\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.126722\n",
      "903\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.115455\n",
      "904\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.114301\n",
      "905\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.113158\n",
      "906\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.122026\n",
      "907\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.130806\n",
      "908\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.129498\n",
      "909\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.128203\n",
      "910\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.136921\n",
      "911\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.145552\n",
      "912\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.144096\n",
      "913\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.122655\n",
      "914\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.111429\n",
      "915\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.090314\n",
      "916\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.079411\n",
      "917\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.088617\n",
      "918\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.097731\n",
      "919\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.086754\n",
      "920\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.085886\n",
      "921\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.065027\n",
      "922\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.064377\n",
      "923\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.043733\n",
      "924\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.053296\n",
      "925\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.052763\n",
      "926\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.052235\n",
      "927\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.061713\n",
      "928\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.061096\n",
      "929\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.070485\n",
      "930\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.069780\n",
      "931\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.059082\n",
      "932\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.058491\n",
      "933\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.057906\n",
      "934\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.067327\n",
      "935\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.056654\n",
      "936\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.056087\n",
      "937\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.045527\n",
      "938\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.055071\n",
      "939\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.064521\n",
      "940\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.073875\n",
      "941\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.063137\n",
      "942\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.042505\n",
      "943\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.052080\n",
      "944\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.041559\n",
      "945\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.041144\n",
      "946\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.040732\n",
      "947\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.040325\n",
      "948\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.009922\n",
      "949\n",
      "resetting env. episode reward total was -18.000000. running mean: -19.989823\n",
      "950\n",
      "resetting env. episode reward total was -20.000000. running mean: -19.989924\n",
      "951\n",
      "resetting env. episode reward total was -20.000000. running mean: -19.990025\n",
      "952\n",
      "resetting env. episode reward total was -20.000000. running mean: -19.990125\n",
      "953\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.000224\n",
      "954\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.010221\n",
      "955\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.020119\n",
      "956\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.019918\n",
      "957\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.029719\n",
      "958\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.039422\n",
      "959\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.049027\n",
      "960\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.038537\n",
      "961\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.048152\n",
      "962\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.057670\n",
      "963\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.057094\n",
      "964\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.056523\n",
      "965\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.065957\n",
      "966\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.065298\n",
      "967\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.064645\n",
      "968\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.073998\n",
      "969\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.083258\n",
      "970\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.092426\n",
      "971\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.081502\n",
      "972\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.080687\n",
      "973\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.079880\n",
      "974\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.089081\n",
      "975\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.078190\n",
      "976\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.087408\n",
      "977\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.096534\n",
      "978\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.095569\n",
      "979\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.104613\n",
      "980\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.093567\n",
      "981\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.082631\n",
      "982\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.091805\n",
      "983\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.090887\n",
      "984\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.089978\n",
      "985\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.099078\n",
      "986\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.108087\n",
      "987\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.117007\n",
      "988\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.115837\n",
      "989\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.114678\n",
      "990\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.123531\n",
      "991\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.122296\n",
      "992\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.111073\n",
      "993\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.119962\n",
      "994\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.118763\n",
      "995\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.127575\n",
      "996\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.126299\n",
      "997\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.135036\n",
      "998\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.143686\n",
      "999\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.152249\n",
      "1000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.140727\n",
      "1001\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.149319\n",
      "1002\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.157826\n",
      "1003\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.126248\n",
      "1004\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.124985\n",
      "1005\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.113736\n",
      "1006\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.122598\n",
      "1007\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.131372\n",
      "1008\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.110059\n",
      "1009\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.088958\n",
      "1010\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.098068\n",
      "1011\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.097088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.106117\n",
      "1013\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.115056\n",
      "1014\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.123905\n",
      "1015\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.132666\n",
      "1016\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.121339\n",
      "1017\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.090126\n",
      "1018\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.089225\n",
      "1019\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.078332\n",
      "1020\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.087549\n",
      "1021\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.086674\n",
      "1022\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.075807\n",
      "1023\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.085049\n",
      "1024\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.094198\n",
      "1025\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.103256\n",
      "1026\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.112224\n",
      "1027\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.121102\n",
      "1028\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.119891\n",
      "1029\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.128692\n",
      "1030\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.127405\n",
      "1031\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.136131\n",
      "1032\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.144769\n",
      "1033\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.153322\n",
      "1034\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.161788\n",
      "1035\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.150171\n",
      "1036\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.118669\n",
      "1037\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.127482\n",
      "1038\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.116207\n",
      "1039\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.125045\n",
      "1040\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.123795\n",
      "1041\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.112557\n",
      "1042\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.101431\n",
      "1043\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.110417\n",
      "1044\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.109313\n",
      "1045\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.118220\n",
      "1046\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.127038\n",
      "1047\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.105767\n",
      "1048\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.094709\n",
      "1049\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.103762\n",
      "1050\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.102725\n",
      "1051\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.111698\n",
      "1052\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.120581\n",
      "1053\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.119375\n",
      "1054\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.128181\n",
      "1055\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.136899\n",
      "1056\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.135530\n",
      "1057\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.144175\n",
      "1058\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.142733\n",
      "1059\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.141306\n",
      "1060\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.119893\n",
      "1061\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.128694\n",
      "1062\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.137407\n",
      "1063\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.136033\n",
      "1064\n",
      "resetting env. episode reward total was -16.000000. running mean: -20.094672\n",
      "1065\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.083726\n",
      "1066\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.092889\n",
      "1067\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.101960\n",
      "1068\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.100940\n",
      "1069\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.109931\n",
      "1070\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.108831\n",
      "1071\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.107743\n",
      "1072\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.116666\n",
      "1073\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.105499\n",
      "1074\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.104444\n",
      "1075\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.103399\n",
      "1076\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.112365\n",
      "1077\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.111242\n",
      "1078\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.110129\n",
      "1079\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.109028\n",
      "1080\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.117938\n",
      "1081\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.116758\n",
      "1082\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.115591\n",
      "1083\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.124435\n",
      "1084\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.123191\n",
      "1085\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.121959\n",
      "1086\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.120739\n",
      "1087\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.129532\n",
      "1088\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.138236\n",
      "1089\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.146854\n",
      "1090\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.135386\n",
      "1091\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.124032\n",
      "1092\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.132791\n",
      "1093\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.141463\n",
      "1094\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.150049\n",
      "1095\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.158548\n",
      "1096\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.166963\n",
      "1097\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.155293\n",
      "1098\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.153740\n",
      "1099\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.142203\n",
      "1100\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.140781\n",
      "1101\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.149373\n",
      "1102\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.127879\n",
      "1103\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.136601\n",
      "1104\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.145234\n",
      "1105\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.143782\n",
      "1106\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.142344\n",
      "1107\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.140921\n",
      "1108\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.119512\n",
      "1109\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.128317\n",
      "1110\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.137033\n",
      "1111\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.145663\n",
      "1112\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.134206\n",
      "1113\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.112864\n",
      "1114\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.101736\n",
      "1115\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.100718\n",
      "1116\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.109711\n",
      "1117\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.108614\n",
      "1118\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.107528\n",
      "1119\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.106453\n",
      "1120\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.105388\n",
      "1121\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.104334\n",
      "1122\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.113291\n",
      "1123\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.112158\n",
      "1124\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.121036\n",
      "1125\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.129826\n",
      "1126\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.118528\n",
      "1127\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.117343\n",
      "1128\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.106169\n",
      "1129\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.095107\n",
      "1130\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.104156\n",
      "1131\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.083115\n",
      "1132\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.092284\n",
      "1133\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.101361\n",
      "1134\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.110347\n",
      "1135\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.119244\n",
      "1136\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.108051\n",
      "1137\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.106971\n",
      "1138\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.115901\n",
      "1139\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.114742\n",
      "1140\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.113595\n",
      "1141\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.122459\n",
      "1142\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.121234\n",
      "1143\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.130022\n",
      "1144\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.138722\n",
      "1145\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.137334\n",
      "1146\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.125961\n",
      "1147\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.124701\n",
      "1148\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.133454\n",
      "1149\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.132120\n",
      "1150\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.130799\n",
      "1151\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.129491\n",
      "1152\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.138196\n",
      "1153\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.146814\n",
      "1154\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.145346\n",
      "1155\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.153892\n",
      "1156\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.162353\n",
      "1157\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.170730\n",
      "1158\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.179022\n",
      "1159\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.187232\n",
      "1160\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.175360\n",
      "1161\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.183606\n",
      "1162\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.181770\n",
      "1163\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.189952\n",
      "1164\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.188053\n",
      "1165\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.196172\n",
      "1166\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.204211\n",
      "1167\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.212169\n",
      "1168\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.220047\n",
      "1169\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.217846\n",
      "1170\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.215668\n",
      "1171\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.223511\n",
      "1172\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.231276\n",
      "1173\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.218963\n",
      "1174\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.226774\n",
      "1175\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.214506\n",
      "1176\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.212361\n",
      "1177\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.220237\n",
      "1178\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.228035\n",
      "1179\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.235755\n",
      "1180\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.233397\n",
      "1181\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.241063\n",
      "1182\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.218653\n",
      "1183\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.226466\n",
      "1184\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.234201\n",
      "1185\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.241859\n",
      "1186\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.249441\n",
      "1187\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.256946\n",
      "1188\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.264377\n",
      "1189\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.261733\n",
      "1190\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.269116\n",
      "1191\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.276425\n",
      "1192\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.283660\n",
      "1193\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.280824\n",
      "1194\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.288016\n",
      "1195\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.295135\n",
      "1196\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.292184\n",
      "1197\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.259262\n",
      "1198\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.246670\n",
      "1199\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.254203\n",
      "1200\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.261661\n",
      "1201\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.259044\n",
      "1202\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.266454\n",
      "1203\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.253789\n",
      "1204\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.251251\n",
      "1205\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.238739\n",
      "1206\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.246351\n",
      "1207\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.253888\n",
      "1208\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.231349\n",
      "1209\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.229036\n",
      "1210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was -21.000000. running mean: -20.236745\n",
      "1211\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.244378\n",
      "1212\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.251934\n",
      "1213\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.229415\n",
      "1214\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.237120\n",
      "1215\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.234749\n",
      "1216\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.242402\n",
      "1217\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.249978\n",
      "1218\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.247478\n",
      "1219\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.255003\n",
      "1220\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.252453\n",
      "1221\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.249929\n",
      "1222\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.257429\n",
      "1223\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.244855\n",
      "1224\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.242407\n",
      "1225\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.219982\n",
      "1226\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.207783\n",
      "1227\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.215705\n",
      "1228\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.203548\n",
      "1229\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.191512\n",
      "1230\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.179597\n",
      "1231\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.177801\n",
      "1232\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.166023\n",
      "1233\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.174363\n",
      "1234\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.182619\n",
      "1235\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.190793\n",
      "1236\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.198885\n",
      "1237\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.206896\n",
      "1238\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.204827\n",
      "1239\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.212779\n",
      "1240\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.210651\n",
      "1241\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.218545\n",
      "1242\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.226359\n",
      "1243\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.234096\n",
      "1244\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.231755\n",
      "1245\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.239437\n",
      "1246\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.247043\n",
      "1247\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.254572\n",
      "1248\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.252027\n",
      "1249\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.259506\n",
      "1250\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.266911\n",
      "1251\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.264242\n",
      "1252\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.271600\n",
      "1253\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.258884\n",
      "1254\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.236295\n",
      "1255\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.243932\n",
      "1256\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.251493\n",
      "1257\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.248978\n",
      "1258\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.256488\n",
      "1259\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.263923\n",
      "1260\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.271284\n",
      "1261\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.278571\n",
      "1262\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.285785\n",
      "1263\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.282928\n",
      "1264\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.290098\n",
      "1265\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.267197\n",
      "1266\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.264525\n",
      "1267\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.261880\n",
      "1268\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.269261\n",
      "1269\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.276569\n",
      "1270\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.253803\n",
      "1271\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.241265\n",
      "1272\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.238852\n",
      "1273\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.246464\n",
      "1274\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.233999\n",
      "1275\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.221659\n",
      "1276\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.229443\n",
      "1277\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.217148\n",
      "1278\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.224977\n",
      "1279\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.222727\n",
      "1280\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.230500\n",
      "1281\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.238195\n",
      "1282\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.235813\n",
      "1283\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.233455\n",
      "1284\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.241120\n",
      "1285\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.248709\n",
      "1286\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.256222\n",
      "1287\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.253659\n",
      "1288\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.241123\n",
      "1289\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.238712\n",
      "1290\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.246325\n",
      "1291\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.253861\n",
      "1292\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.251323\n",
      "1293\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.248809\n",
      "1294\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.256321\n",
      "1295\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.233758\n",
      "1296\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.241421\n",
      "1297\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.239006\n",
      "1298\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.246616\n",
      "1299\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.244150\n",
      "1300\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.231709\n",
      "1301\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.239392\n",
      "1302\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.246998\n",
      "1303\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.244528\n",
      "1304\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.232082\n",
      "1305\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.229762\n",
      "1306\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.237464\n",
      "1307\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.245089\n",
      "1308\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.252638\n",
      "1309\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.260112\n",
      "1310\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.267511\n",
      "1311\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.264836\n",
      "1312\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.262187\n",
      "1313\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.269566\n",
      "1314\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.256870\n",
      "1315\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.234301\n",
      "1316\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.231958\n",
      "1317\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.229639\n",
      "1318\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.237342\n",
      "1319\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.234969\n",
      "1320\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.242619\n",
      "1321\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.230193\n",
      "1322\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.237891\n",
      "1323\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.245512\n",
      "1324\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.253057\n",
      "1325\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.250526\n",
      "1326\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.248021\n",
      "1327\n",
      "resetting env. episode reward total was -17.000000. running mean: -20.215541\n",
      "1328\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.223385\n",
      "1329\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.211152\n",
      "1330\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.209040\n",
      "1331\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.216950\n",
      "1332\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.214780\n",
      "1333\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.222632\n",
      "1334\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.210406\n",
      "1335\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.218302\n",
      "1336\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.226119\n",
      "1337\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.213858\n",
      "1338\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.211719\n",
      "1339\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.219602\n",
      "1340\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.227406\n",
      "1341\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.205132\n",
      "1342\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.213081\n",
      "1343\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.210950\n",
      "1344\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.208840\n",
      "1345\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.216752\n",
      "1346\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.224584\n",
      "1347\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.222339\n",
      "1348\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.210115\n",
      "1349\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.218014\n",
      "1350\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.225834\n",
      "1351\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.233576\n",
      "1352\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.221240\n",
      "1353\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.229027\n",
      "1354\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.236737\n",
      "1355\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.244370\n",
      "1356\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.251926\n",
      "1357\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.239407\n",
      "1358\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.227013\n",
      "1359\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.214743\n",
      "1360\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.212595\n",
      "1361\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.200469\n",
      "1362\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.188465\n",
      "1363\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.196580\n",
      "1364\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.184614\n",
      "1365\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.172768\n",
      "1366\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.161040\n",
      "1367\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.159430\n",
      "1368\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.157836\n",
      "1369\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.146257\n",
      "1370\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.154795\n",
      "1371\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.163247\n",
      "1372\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.171614\n",
      "1373\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.169898\n",
      "1374\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.168199\n",
      "1375\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.176517\n",
      "1376\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.164752\n",
      "1377\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-d8e515b97638>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[0mepisode_hidden_layer_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_observations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_gradient_log_ps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# reset values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m             \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# reset env\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m             \u001b[0mrunning_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward_sum\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrunning_reward\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrunning_reward\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.99\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreward_sum\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'resetting env. episode reward total was %f. running mean: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreward_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\saran\\appdata\\local\\conda\\conda\\envs\\tensor\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\saran\\appdata\\local\\conda\\conda\\envs\\tensor\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;31m# return: (states, observations)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\saran\\appdata\\local\\conda\\conda\\envs\\tensor\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mreset_game\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetLegalActionSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SORN2013_10_Exp.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
