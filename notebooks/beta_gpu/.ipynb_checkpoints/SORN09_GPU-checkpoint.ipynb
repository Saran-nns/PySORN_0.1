{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1538221438238,
     "user": {
      "displayName": "Saran N Subramaniyan",
      "photoUrl": "",
      "userId": "16986668814671639642"
     },
     "user_tz": -330
    },
    "id": "9nE596N3uTmH",
    "outputId": "edc9c9b5-b7ed-47f9-e2a4-0b614868ecfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.7 |Anaconda, Inc.| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check python version\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GL5qZSNEb8Wy"
   },
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DrilDOPCvXeV"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import random\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import pickle\n",
    "import torch\n",
    "import torch.distributions as tdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the default tensor type at the top\n",
    "# torch.set_default_tensor_type(torch.cuda.FloatTensor if torch.cuda.is_available() \n",
    "#                                                      else torch.FloatTensor)\n",
    "\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aj91VUcXdSO8"
   },
   "source": [
    "### VERIFY INSTALLED TORCH VERSION AND AVAILABLE GPU DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1538221604638,
     "user": {
      "displayName": "Saran N Subramaniyan",
      "photoUrl": "",
      "userId": "16986668814671639642"
     },
     "user_tz": -330
    },
    "id": "-2meklAE3ZNH",
    "outputId": "2d9e9b65-89fd-4632-aac7-11e0f6f84204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available devices: 1\n",
      "Current device: 0\n",
      "Current device name: GeForce GTX 1060 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "torch.__version__\n",
    "print('Number of available devices:',torch.cuda.device_count())\n",
    "print('Current device:',torch.cuda.current_device())\n",
    "print('Current device name:', torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set all tensors as DoubleTensor by default\n",
    "# torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "# As recommended by Pytorch Devs: use float32 instead of double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNWADtl2viZn"
   },
   "outputs": [],
   "source": [
    "### WHITE GAUSSIAN NOISE GENERATOR\n",
    "\n",
    "def white_gaussian_noise(mu, sigma,t):\n",
    "\n",
    "    \"\"\"Generates white gaussian noise with mean mu, standard deviation sigma and\n",
    "    the noise length equals t \"\"\"\n",
    "    \n",
    "    n = tdist.Normal(mu, sigma)\n",
    "    noise = n.sample((t,))\n",
    "    \n",
    "    return noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBPiSadA7U3L"
   },
   "source": [
    "### UTIL FUNCTION TO GENERATE LAMBDA CONNECTION MATRICES \n",
    "\n",
    "#### Util function is called only during initialization; Doesnt require GPU support\n",
    "#### Written using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msYVL6Nv7tJS"
   },
   "outputs": [],
   "source": [
    "# Implement lambda incoming and outgoing connections per neuron\n",
    "\n",
    "def generate_lambd_connections(synaptic_connection,ne,ni, lambd_w,lambd_std):\n",
    "    \n",
    "    \n",
    "    if synaptic_connection == 'EE':\n",
    "        \n",
    "        \n",
    "        \"\"\"Choose random lamda connections per neuron\"\"\"\n",
    "\n",
    "        # Draw normally distribued ne integers with mean lambd_w\n",
    "\n",
    "        lambdas_incoming = norm.ppf(np.random.random(ne), loc=lambd_w*2 + 0.5, scale=lambd_std).astype(int)\n",
    "        \n",
    "        # lambdas_outgoing = norm.ppf(np.random.random(ne), loc=lambd_w, scale=lambd_std).astype(int)\n",
    "    \n",
    "        # List of neurons \n",
    "\n",
    "        list_neurons= list(range(ne))\n",
    "\n",
    "        # Connection weights\n",
    "\n",
    "        connection_weights = np.zeros((ne,ne))\n",
    "\n",
    "        # For each lambd value in the above list,\n",
    "\n",
    "        # Choose the neurons in order [0 to 199]\n",
    "\n",
    "        for neuron in list_neurons:\n",
    "\n",
    "            ### Choose ramdom unique (lambdas[neuron]) neurons from  list_neurons\n",
    "            possible_connections = list_neurons.copy()\n",
    "            \n",
    "            possible_connections.remove(neuron)  # Remove the selected neuron from possible connections i!=j\n",
    "\n",
    "            possible_incoming_connections = random.sample(possible_connections,lambdas_incoming[neuron])  \n",
    "\n",
    "            # Generate weights for incoming and outgoing connections\n",
    "            \n",
    "            # Gaussian Distribution of weights \n",
    "            \n",
    "            # weight_matrix = np.random.randn(Sorn.ne, Sorn.ni) + 2 # Small random values from gaussian distribution\n",
    "            # Centered around 2 to make all values positive \n",
    "            # weight_matrix*=0.01 # Setting spectral radius \n",
    "            \n",
    "            # Uniform Distribution: Weights are drawn randomly from the interval [0,1]\n",
    "            \n",
    "            incoming_weights  = np.random.uniform(0.0,0.1,lambdas_incoming[neuron]) \n",
    "        \n",
    "            # ---------- Update the connection weight matrix ------------\n",
    "\n",
    "            # Update incoming connection weights for selected 'neuron'\n",
    "\n",
    "            for incoming_idx,incoming in enumerate(incoming_weights):    # Update the rows  \n",
    "                connection_weights[possible_incoming_connections[incoming_idx]][neuron] = incoming_weights[incoming_idx]\n",
    "\n",
    "        return connection_weights\n",
    "    \n",
    "    if synaptic_connection == 'EI':\n",
    "        \n",
    "        \"\"\"Choose random lamda connections per neuron\"\"\"\n",
    "\n",
    "        # Draw normally distribued ni integers with mean lambd_w\n",
    "            \n",
    "        lambdas = norm.ppf(np.random.random(ni), loc=lambd_w*2 + 1, scale=lambd_std).astype(int)\n",
    "        \n",
    "        # List of neurons \n",
    "\n",
    "        list_neurons= list(range(ni))  # Each i can connect with random ne neurons \n",
    "\n",
    "        # Connection weights\n",
    "\n",
    "        connection_weights = np.zeros((ni,ne))\n",
    "\n",
    "        # For each lambd value in the above list,\n",
    "\n",
    "        # Choose the neurons in order [0 to 40]\n",
    "\n",
    "        for neuron in list_neurons:\n",
    "\n",
    "            ### Choose ramdom unique (lambdas[neuron]) neurons from  list_neurons\n",
    "            possible_connections = list(range(ne))\n",
    "            # possible_connections.remove(neuron)  # Remove the selected neuron from possible connections i!=j\n",
    "\n",
    "            # possible_incoming_connections = random.sample(possible_connections,lambdas[neuron])  # possible_incoming connections to the slected neuron\n",
    "            possible_outgoing_connections = random.sample(possible_connections,lambdas[neuron])  # possible_outgoing connections to the neuron\n",
    "\n",
    "            # Generate weights for incoming and outgoing connections\n",
    "            # Weights are drawn randomly from the interval [0,1]\n",
    "\n",
    "            # incoming_weights  = np.random.random(lambdas[neuron]) # Incomig connections are column in the connection_weights matrix\n",
    "            outgoing_weights = np.random.uniform(0.0,0.1,lambdas[neuron])  # Outgoing connections are rows in the connection_weights matrix\n",
    "\n",
    "            # ---------- Update the connection weight matrix ------------\n",
    "\n",
    "            # Update outgoing connections for the neuron\n",
    "\n",
    "            for outgoing_idx,outgoing_weight in enumerate(outgoing_weights):  # Update the columns in the connection matrix\n",
    "                connection_weights[neuron][possible_outgoing_connections[outgoing_idx]] = outgoing_weight\n",
    "        \n",
    "        return connection_weights\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3vCXDMZk89qg"
   },
   "source": [
    "### SANITY CHECK EACH WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rtg5EpTWEA_n"
   },
   "outputs": [],
   "source": [
    "### NOTE REQUIRED; REFER CPU CODES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bb9PctPiEBVB"
   },
   "source": [
    "### HELPER FUNCTION TO NORMALIZE INCOMING WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6EN8aBruEBoA"
   },
   "outputs": [],
   "source": [
    "def normalize_weight_matrix(weight_matrix):\n",
    "    \n",
    "    # Applied only while initializing the weight. Later Synaptic scalling applied on weight matrices\n",
    "    \n",
    "    \"\"\" Normalize the weights in the matrix such that incoming connections to a neuron sum up to 1\n",
    "    \n",
    "    Args:\n",
    "        weight_matrix(array) -- Incoming Weights from W_ee or W_ei or W_ie\n",
    "    \n",
    "    Returns:\n",
    "        weight_matrix(array) -- Normalized weight matrix\"\"\"\n",
    "\n",
    "    normalized_weight_matrix = weight_matrix / np.sum(weight_matrix,axis = 0)\n",
    "\n",
    "    return normalized_weight_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_glyj4U5C8f"
   },
   "source": [
    "### SORN INITIALIZING CLASS:\n",
    "#### Init all required matrices for the network simulation and analysis\n",
    "#### NOTE: \n",
    "#### This class is called only once and it has no influence during simulation\n",
    "#### Hence, it is just build with numpy. Later its matrices are converted into torchable tensors during simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8M0p9dckwlR8"
   },
   "outputs": [],
   "source": [
    "class Sorn(object):\n",
    "    \n",
    "    \"\"\"SORN 2 network model Initialization\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\"Initialize network variables as class variables of SORN\"\"\"\n",
    "    \n",
    "    nu = 10                    # Number of input units\n",
    "    ne = 200                   # Number of excitatory units\n",
    "    ni = int(0.2*ne)           # Number of inhibitory units in the network\n",
    "    eta_stdp = 0.004\n",
    "    eta_inhib = 0.001\n",
    "    eta_ip = 0.01\n",
    "    te_max = 1.0 \n",
    "    ti_max = 0.5\n",
    "    ti_min = 0.0\n",
    "    te_min = 0.0\n",
    "    mu_ip = 0.1\n",
    "    sigma_ip  = 0.0 # Standard deviation, variance == 0 \n",
    "    \n",
    "    \n",
    "    # Initialize weight matrices\n",
    "\n",
    "    def initialize_weight_matrix(self, network_type,synaptic_connection, self_connection, lambd_w): \n",
    "\n",
    "\n",
    "        if (network_type == \"Sparse\") and (self_connection == \"False\"):\n",
    "\n",
    "            \"\"\"Generate weight matrix for E-E/ E-I connections with mean lamda incoming and outgiong connections per neuron\"\"\"\n",
    "            \n",
    "            weight_matrix = generate_lambd_connections(synaptic_connection,Sorn.ne,Sorn.ni,lambd_w,lambd_std = 1)\n",
    "        \n",
    "        # Dense matrix for W_ie\n",
    "\n",
    "        elif (network_type == 'Dense') and (self_connection == 'False'):\n",
    "\n",
    "            # Gaussian distribution of weights\n",
    "            # weight_matrix = np.random.randn(Sorn.ne, Sorn.ni) + 2 # Small random values from gaussian distribution\n",
    "            # Centered around 1 \n",
    "            # weight_matrix.reshape(Sorn.ne, Sorn.ni) \n",
    "            # weight_matrix *= 0.01 # Setting spectral radius \n",
    "            \n",
    "            # Uniform distribution of weights\n",
    "            weight_matrix = np.random.uniform(0.0,0.1,(Sorn.ne, Sorn.ni))\n",
    "            weight_matrix.reshape((Sorn.ne,Sorn.ni))\n",
    "\n",
    "        return weight_matrix\n",
    "\n",
    "    def initialize_threshold_matrix(self, te_min,te_max, ti_min,ti_max):\n",
    "\n",
    "        # Initialize the threshold for excitatory and inhibitory neurons\n",
    "        \n",
    "        \"\"\"Args:\n",
    "            te_min(float) -- Min threshold value for excitatory units\n",
    "            ti_min(float) -- Min threshold value for inhibitory units\n",
    "            te_max(float) -- Max threshold value for excitatory units\n",
    "            ti_max(float) -- Max threshold value for inhibitory units\n",
    "        Returns:\n",
    "            te(vector) -- Threshold values for excitatory units\n",
    "            ti(vector) -- Threshold values for inhibitory units\"\"\"\n",
    "\n",
    "        te = np.random.uniform(0., te_max, (Sorn.ne, 1))\n",
    "        ti = np.random.uniform(0., ti_max, (Sorn.ni, 1))\n",
    "\n",
    "        return te, ti\n",
    "\n",
    "    def initialize_activity_vector(self,ne, ni):\n",
    "        \n",
    "        # Initialize the activity vectors X and Y for excitatory and inhibitory neurons\n",
    "        \n",
    "        \"\"\"Args:\n",
    "            ne(int) -- Number of excitatory neurons\n",
    "            ni(int) -- Number of inhibitory neurons\n",
    "        Returns:\n",
    "             x(array) -- Array of activity vectors of excitatory population\n",
    "             y(array) -- Array of activity vectors of inhibitory population\"\"\"\n",
    "\n",
    "        x = np.zeros((ne, 2))\n",
    "        y = np.zeros((ni, 2))\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UnS-OUC6vDH"
   },
   "source": [
    "### INITIALIZE MATRICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 471,
     "status": "ok",
     "timestamp": 1538223144354,
     "user": {
      "displayName": "Saran N Subramaniyan",
      "photoUrl": "",
      "userId": "16986668814671639642"
     },
     "user_tz": -330
    },
    "id": "ArzF0ULv6tR8",
    "outputId": "46295a92-9a0f-43be-ef74-4b69d62c1c89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4003 1617 8000\n"
     ]
    }
   ],
   "source": [
    "# Create and initialize sorn object and varaibles\n",
    "\n",
    "sorn_init = Sorn()   # Sorn instance only for matrix initialization\n",
    "WEE_init = sorn_init.initialize_weight_matrix(network_type='Sparse',synaptic_connection = 'EE', self_connection='False',lambd_w = 10)\n",
    "WEI_init = sorn_init.initialize_weight_matrix(network_type='Sparse',synaptic_connection = 'EI', self_connection='False',lambd_w = 20)\n",
    "WIE_init = sorn_init.initialize_weight_matrix(network_type='Dense',synaptic_connection = 'IE', self_connection='False',lambd_w = None)\n",
    "\n",
    "\n",
    "Wee_init = WEE_init.copy()\n",
    "Wei_init = WEI_init.copy()\n",
    "Wie_init = WIE_init.copy()\n",
    "\n",
    "c = np.count_nonzero(Wee_init)  # Max: 39800; Target: 3980\n",
    "v = np.count_nonzero(Wei_init)  # Max: 8000; target : 1600  : \n",
    "b = np.count_nonzero(Wie_init)  # Max: 8000; Target: 8000\n",
    "\n",
    "print(c,v,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fsoSwlQaDk3t"
   },
   "source": [
    "### NORMALIZE THE INCOMING WEIGHTS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjV9ape3DlJR"
   },
   "outputs": [],
   "source": [
    "# Normaalize the incoming weights i.e sum(incoming weights to a neuron) = 1\n",
    "\n",
    "wee_init = normalize_weight_matrix(Wee_init)\n",
    "wei_init = normalize_weight_matrix(Wei_init) \n",
    "wie_init = normalize_weight_matrix(Wie_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-3t6YGAM1O3"
   },
   "source": [
    "### Rest of the code needs to build in torch using available Cuda device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvHnW3UKNAs2"
   },
   "source": [
    "### Check CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1538223147948,
     "user": {
      "displayName": "Saran N Subramaniyan",
      "photoUrl": "",
      "userId": "16986668814671639642"
     },
     "user_tz": -330
    },
    "id": "zUtHm98cNGZQ",
    "outputId": "e01e7d91-5a31-4cd3-8bcc-69c3fa0bc758"
   },
   "outputs": [],
   "source": [
    "# use_cuda = torch.cuda.is_available()     # True\n",
    "# n_gpus = torch.cuda.device_count()    # 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ed7d4-SWj7Yp"
   },
   "source": [
    "### Use the default GPU device for all variables defined as tensor float data types\n",
    "#### Note this feaature is added in Torch 0.4 version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7dq_RPzgj7pZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "thKmlAQU63hi"
   },
   "source": [
    "### CONVERT INITIALIZED WEIGHT MATRICES INTO TORCHABLE TENSORS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8pduGmbP64W_"
   },
   "outputs": [],
   "source": [
    "wee_init = torch.from_numpy(wee_init).cuda()\n",
    "wei_init = torch.from_numpy(wei_init).cuda()\n",
    "wie_init = torch.from_numpy(wie_init).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI8anyFt-2Ru"
   },
   "source": [
    "#### INITIALIZE THRESHOLD AND ACTIVITY MATRICES AND CONVERT THEM INTO TENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1nuKzvN-2kU"
   },
   "outputs": [],
   "source": [
    "te_init, ti_init = sorn_init.initialize_threshold_matrix(Sorn.te_min,Sorn.te_max,Sorn.ti_min,Sorn.ti_max)\n",
    "x_init, y_init = sorn_init.initialize_activity_vector(Sorn.ne, Sorn.ni)\n",
    "\n",
    "te_init,ti_init = torch.from_numpy(te_init),torch.from_numpy(ti_init).cuda()\n",
    "x_init,y_init = torch.from_numpy(x_init),torch.from_numpy(y_init).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DQDua5T3eJml"
   },
   "source": [
    "# Helpers functions with GPU support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yUtb9qCzeJ5v"
   },
   "outputs": [],
   "source": [
    "# Helpers for Plasticity.stdp()\n",
    "\n",
    "def prune_small_weights(weights,cutoff_weight):\n",
    "    \n",
    "    \"\"\" Prune the connections with negative connection strength\"\"\"\n",
    "    \n",
    "    # No need to define tensor data types explicitly since default tensor types \n",
    "    # are already set using the execution line above -  From Torch version 0.4 \n",
    "    \n",
    "    weights[weights <= cutoff_weight] = cutoff_weight\n",
    "    \n",
    "    return weights\n",
    "    \n",
    "\n",
    "def set_max_cutoff_weight(weights, cutoff_weight):\n",
    "    \n",
    "    \"\"\" Set cutoff limit for the values in given array\"\"\"\n",
    "    \n",
    "    weights[weights > cutoff_weight] = cutoff_weight\n",
    "    \n",
    "    return weights\n",
    "  \n",
    "# Helper for Plasticity.ss()\n",
    "\n",
    "def get_unconnected_indexes(wee):\n",
    "    \n",
    "    \"\"\"\n",
    "    Helper function for Structural plasticity to randomly select the unconnected units\n",
    "    \n",
    "    Args: \n",
    "    wee -  Weight matrix\n",
    "    \n",
    "    Returns:\n",
    "    list (indices) // indices = (row_idx,col_idx)\"\"\"\n",
    "    \n",
    "\n",
    "    i,j = torch.where(wee <= 0.).cuda()\n",
    "    indices = list(zip(i,j))\n",
    "    \n",
    "    self_conn_removed = []\n",
    "    for i,idxs in enumerate(indices):\n",
    "        \n",
    "        if idxs[0] != idxs[1]:\n",
    "            \n",
    "            self_conn_removed.append(indices[i])\n",
    "    \n",
    "    return self_conn_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Y_wSyvSGqc8"
   },
   "source": [
    "### PLASTICITY CLASS (Child of Class SORN) written in torch ; Instances of this class will be called iteratively during simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iSwQY3BwGqvo"
   },
   "outputs": [],
   "source": [
    "class Plasticity(Sorn):\n",
    "    \"\"\"\n",
    "    Instance of class Sorn. Inherits the variables and functions defined in class Sorn\n",
    "    Encapsulates all plasticity mechanisms mentioned in the article \"\"\"\n",
    "\n",
    "    # Initialize the global variables for the class //Class attributes\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.nu = torch.Tensor([Sorn.nu]).cuda() # Number of input units\n",
    "        self.ne = torch.Tensor([Sorn.ne]).cuda()  # Number of excitatory units\n",
    "        self.eta_stdp = torch.Tensor([Sorn.eta_stdp]).cuda()  # Learning rate in other words\n",
    "        self.eta_ip = torch.Tensor([Sorn.eta_ip]).cuda()  \n",
    "        self.eta_inhib = torch.Tensor([Sorn.eta_inhib]).cuda() \n",
    "        self.h_ip = torch.Tensor([2]) * torch.Tensor([Sorn.nu]) / torch.Tensor([Sorn.ne])\n",
    "        self.mu_ip = torch.Tensor([Sorn.mu_ip]).cuda()\n",
    "        self.ni = torch.Tensor([Sorn.ni]).cuda()\n",
    "        self.time_steps = torch.Tensor([Sorn.time_steps]).cuda()\n",
    "        self.te_min = torch.Tensor([Sorn.te_min]).cuda()\n",
    "        self.te_max = torch.Tensor([Sorn.te_max]).cuda()\n",
    "        \n",
    "    def stdp(self, wee, x, cutoff_weights):\n",
    "        \n",
    "        \"\"\" Apply STDP rule : Regulates synaptic strength between the pre(Xj) and post(Xi) synaptic neurons\"\"\"\n",
    "\n",
    "        xt_1 = x[:,0].unsqueeze(1)\n",
    "        xt = x[:,1].unsqueeze(1)\n",
    "        wee_t = wee.clone().cuda() \n",
    "\n",
    "        # STDP applies only on the neurons which are connected.\n",
    "\n",
    "        for i in range(len(wee_t[0])): # Each neuron i, Post-synaptic neuron \n",
    "\n",
    "            for j in range(len(wee_t[0:])): # Incoming connection from jth pre-synaptic neuron to ith neuron\n",
    "\n",
    "                if wee_t[j][i] != 0. : # Check connectivity\n",
    "\n",
    "                    # Get the change in weight\n",
    "                    delta_wee_t = self.eta_stdp * (xt[i] * xt_1[j] - xt_1[i]*xt[j]).cuda()\n",
    "\n",
    "                    # Update the weight between jth neuron to i \"\"Different from notation in article \n",
    "\n",
    "                    wee_t[j][i] = wee[j][i].cuda() + delta_wee_t.cuda()\n",
    "\n",
    "        \"\"\" Prune the smallest weights induced by plasticity mechanisms; Apply lower cutoff weight\"\"\"\n",
    "        wee_t = prune_small_weights(wee_t,cutoff_weights[0])\n",
    "\n",
    "        \"\"\"Check and set all weights < upper cutoff weight \"\"\"\n",
    "        wee_t = set_max_cutoff_weight(wee_t,cutoff_weights[1])\n",
    "\n",
    "        return wee_t\n",
    "\n",
    "    def ip(self, te, x):\n",
    "        \n",
    "        # IP rule: Active unit increases its threshold and inactive decreases its threshold.\n",
    "\n",
    "        xt = x[:, 1].unsqueeze(1)\n",
    "\n",
    "        te_update = te.cuda() + self.eta_ip.cuda() * (xt.cuda() - self.h_ip.cuda())\n",
    "        \n",
    "        \"\"\" Check whether all te are in range [0.0,1.0] and update acordingly\"\"\"\n",
    "        \n",
    "        # Update te < 0.0 ---> 0.0  # Would be nice to have separate function name for thresholds\n",
    "        # te_update = prune_small_weights(te_update,self.te_min)\n",
    "        \n",
    "        # Set all te > 1.0 --> 1.0\n",
    "        # te_update = set_max_cutoff_weight(te_update,self.te_max)\n",
    "\n",
    "        return te_update\n",
    "\n",
    "    def ss(self, wee_t):\n",
    "        \n",
    "        \"\"\"Synaptic Scaling or Synaptic Normalization\"\"\"\n",
    "        \n",
    "        wee_t = torch.div(wee_t, torch.sum(wee_t,dim=0)).cuda()\n",
    "\n",
    "        return wee_t\n",
    "\n",
    "    \n",
    "    def istdp(self, wei, x, y, cutoff_weights):\n",
    "\n",
    "        #  Apply iSTDP rule : Regulates synaptic strength between the pre(Yj) and post(Xi) synaptic neurons\n",
    "        \n",
    "        xt_1 = x[:, 0].unsqueeze(1)\n",
    "        xt = x[:, 1].unsqueeze(1)\n",
    "    \n",
    "        yt_1 = y[:, 0].unsqueeze(1)    \n",
    "        yt = y[:, 1].unsqueeze(1)  \n",
    " \n",
    "\n",
    "        # iSTDP applies only on the neurons which are connected.\n",
    "        wei_t = wei.clone()\n",
    "\n",
    "        for i in range(len(wei_t[0])): # Each neuron i, Post-synaptic neuron: means for each column; \n",
    "            \n",
    "            for j in range(len(wei_t[0:])): # Incoming connection from j, pre-synaptic neuron to ith neuron\n",
    "                \n",
    "                if wei_t[j][i] != 0. : # Check connectivity\n",
    "                    \n",
    "                    # Get the change in weight\n",
    "                    delta_wei_t = - self.eta_inhib * yt_1[j] * (1 - xt[i]*(1 + 1/self.mu_ip))\n",
    "\n",
    "                    # Update the weight between jth neuron to i \"\"Different from notation in article \n",
    "\n",
    "                    wei_t[j][i] = wei[j][i] + delta_wei_t\n",
    "        \n",
    "        \"\"\" Prune the smallest weights induced by plasticity mechanisms; Apply lower cutoff weight\"\"\"\n",
    "        wei_t = prune_small_weights(wei_t,cutoff_weights[0])\n",
    "        \n",
    "        \"\"\"Check and set all weights < upper cutoff weight \"\"\"\n",
    "        wei_t = set_max_cutoff_weight(wei_t,cutoff_weights[1])\n",
    "        \n",
    "        return wei_t\n",
    "\n",
    "    @staticmethod\n",
    "    \n",
    "    def structural_plasticity(wee):\n",
    "\n",
    "        \"\"\" Add new connection value to the smallest weight between excitatory units randomly\"\"\"\n",
    "        \n",
    "        p_c = torch.randint(0, 10, 1).cuda()\n",
    "\n",
    "        if p_c == 0:  # p_c= 0.1\n",
    "\n",
    "            \"\"\" Do structural plasticity \"\"\"\n",
    "\n",
    "            # Choose the smallest weights randomly from the weight matrix wee\n",
    "            \n",
    "            indexes = get_unconnected_indexes(wee) \n",
    "\n",
    "            # Choose any idx randomly\n",
    "            idx_rand = random.choice(indexes)\n",
    "            \n",
    "            if idx_rand[0] == idx_rand[1]:\n",
    "                \n",
    "                idx_rand = random.choice(indexes)\n",
    "                \n",
    "            wee[idx_rand[0]][idx_rand[1]] = 0.001\n",
    "            \n",
    "\n",
    "        return wee\n",
    "\n",
    "    ###########################################################\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_plasticity():\n",
    "\n",
    "        wee = wee_init\n",
    "        wei = wei_init\n",
    "        wie = wie_init\n",
    "        te = te_init\n",
    "        ti = ti_init\n",
    "        x = x_init\n",
    "        y = y_init\n",
    "\n",
    "        return wee, wei, wie, te, ti, x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def reorganize_network():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPymiaGM2qKB"
   },
   "source": [
    "### MATRIX COLLECTION: Child of class SORN : All other classes store and retrieve the arrays during simulation using this class ; Literally the Memory of SORN\n",
    "\n",
    "No need to change any steps here: Performs storage and retrieval of arrays; Use CPUs; Higher the memory of CPU, longer single simulation step can be! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jLzLUAON2qiX"
   },
   "outputs": [],
   "source": [
    "class MatrixCollection(Sorn):\n",
    "    def __init__(self,phase, matrices = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.phase = phase\n",
    "        self.matrices = matrices\n",
    "        if self.phase == 'Plasticity' and self.matrices == None :\n",
    "\n",
    "            self.time_steps = Sorn.time_steps + 1  # Total training steps\n",
    "            self.Wee, self.Wei, self.Wie, self.Te, self.Ti, self.X, self.Y = [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps\n",
    "            wee, wei, wie, te, ti, x, y = Plasticity.initialize_plasticity()\n",
    "\n",
    "            # Assign initial matrix to the master matrices\n",
    "            self.Wee[0] = wee\n",
    "            self.Wei[0] = wei\n",
    "            self.Wie[0] = wie\n",
    "            self.Te[0] = te\n",
    "            self.Ti[0] = ti\n",
    "            self.X[0] = x\n",
    "            self.Y[0] = y\n",
    "        \n",
    "        elif self.phase == 'Plasticity' and self.matrices != None:\n",
    "            \n",
    "            self.time_steps = Sorn.time_steps + 1  # Total training steps\n",
    "            self.Wee, self.Wei, self.Wie, self.Te, self.Ti, self.X, self.Y = [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps\n",
    "            # Assign matrices from plasticity phase to the new master matrices for training phase\n",
    "            self.Wee[0] = matrices['Wee']\n",
    "            self.Wei[0] = matrices['Wei']\n",
    "            self.Wie[0] = matrices['Wie']\n",
    "            self.Te[0] = matrices['Te']\n",
    "            self.Ti[0] = matrices['Ti']\n",
    "            self.X[0] = matrices['X']\n",
    "            self.Y[0] = matrices['Y']\n",
    "            \n",
    "        elif self.phase == 'Training':\n",
    "\n",
    "            \"\"\"NOTE:\n",
    "            time_steps here is diferent for plasticity or trianing phase\"\"\"\n",
    "            self.time_steps = Sorn.time_steps + 1  # Total training steps\n",
    "            self.Wee, self.Wei, self.Wie, self.Te, self.Ti, self.X, self.Y = [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps, [0] * self.time_steps, \\\n",
    "                                                                             [0] * self.time_steps\n",
    "            # Assign matrices from plasticity phase to new respective matrices for training phase\n",
    "            self.Wee[0] = matrices['Wee']\n",
    "            self.Wei[0] = matrices['Wei']\n",
    "            self.Wie[0] = matrices['Wie']\n",
    "            self.Te[0] = matrices['Te']\n",
    "            self.Ti[0] = matrices['Ti']\n",
    "            self.X[0] = matrices['X']\n",
    "            self.Y[0] = matrices['Y']\n",
    "            \n",
    "    # @staticmethod\n",
    "    def weight_matrix(self, wee, wei, wie, i):\n",
    "        # Get delta_weight from Plasticity.stdp\n",
    "    \n",
    "        # i - training step\n",
    "        self.Wee[i + 1] = wee\n",
    "        self.Wei[i + 1] = wei\n",
    "        self.Wie[i + 1] = wie\n",
    "\n",
    "        return self.Wee, self.Wei, self.Wie\n",
    "\n",
    "    # @staticmethod\n",
    "    def threshold_matrix(self, te, ti, i):\n",
    "        self.Te[i + 1] = te\n",
    "        self.Ti[i + 1] = ti\n",
    "        return self.Te, self.Ti\n",
    "\n",
    "    # @staticmethod\n",
    "    def network_activity_t(self, excitatory_net, inhibitory_net, i):\n",
    "        self.X[i + 1] = excitatory_net\n",
    "        self.Y[i + 1] = inhibitory_net\n",
    "\n",
    "        return self.X, self.Y\n",
    "\n",
    "    # @staticmethod\n",
    "    def network_activity_t_1(self, x, y, i):\n",
    "        x_1, y_1 = [0] * self.time_steps, [0] * self.time_steps\n",
    "        x_1[i] = x\n",
    "        y_1[i] = y\n",
    "\n",
    "        return x_1, y_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwLCCNgq4FSU"
   },
   "source": [
    "### NETWORK STATE: Class which measure the evolution of network state with and without external stimuli. Also reveals recurrent activity in the network. Compute using GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bvy9NWge4Fmp"
   },
   "outputs": [],
   "source": [
    "class NetworkState(Plasticity):\n",
    "    \n",
    "    \"\"\"The evolution of network states\"\"\"\n",
    "\n",
    "    def __init__(self, v_t):\n",
    "        super().__init__()\n",
    "        self.v_t = v_t\n",
    "    \n",
    "    def incoming_drive(self,weights,activity_vector):\n",
    "            \n",
    "        # Broadcasting weight*acivity vectors \n",
    "        \n",
    "        if weights.shape[0] == weights.shape[1]:\n",
    "            incoming = torch.mm(weights.cuda(), activity_vector.cuda())\n",
    "        else:\n",
    "            incoming = torch.mm(weights.t().cuda(), activity_vector.cuda())\n",
    "        incoming = incoming.sum(dim=0).cuda()\n",
    "        return incoming\n",
    "\n",
    "        \n",
    "    def excitatory_network_state(self, wee, wei, te, x, y,white_noise_e):\n",
    "        \n",
    "        \"\"\" Activity of Excitatory neurons in the network\"\"\"\n",
    "    \n",
    "        xt = x[:, 1].unsqueeze(1)  \n",
    "        \n",
    "        yt = y[:, 1].unsqueeze(1)\n",
    "        \n",
    "        incoming_drive_e = self.incoming_drive(weights = wee,activity_vector=xt).cuda()\n",
    "        incoming_drive_i = self.incoming_drive(weights = wei,activity_vector=yt).cuda()\n",
    "        \n",
    "        # tot_incoming_drive = incoming_drive_e.add_(torch.randn(incoming_drive_e.size()) * 0.04) -  incoming_drive_i - te\n",
    "        \n",
    "        tot_incoming_drive = incoming_drive_e -  incoming_drive_i - te.cuda()\n",
    "        \n",
    "        \"\"\"Heaviside step function\"\"\"\n",
    "\n",
    "        heaviside_step = torch.zeros(len(tot_incoming_drive))\n",
    "        for t in range(len(tot_incoming_drive)):\n",
    "            heaviside_step[t] = torch.Tensor([0.0]).cuda() if tot_incoming_drive[t].cuda() < te[t].cuda() else torch.Tensor([1.0]).cuda()\n",
    "\n",
    "        xt_next = heaviside_step.unsqueeze(1).cuda()\n",
    "\n",
    "        return xt_next\n",
    "\n",
    "    def inhibitory_network_state(self, wie, ti, x,white_noise_i):\n",
    "\n",
    "        # Activity of inhibitory neurons\n",
    "\n",
    " \n",
    "        xt = x[:, 1].unsqueeze(1)\n",
    "        \n",
    "    \n",
    "        incoming_drive_i = self.incoming_drive(weights = wie,activity_vector=xt).cuda()\n",
    "        \n",
    "#         tot_incoming_drive = incoming_drive_i.add_(torch.randn(incoming_drive_e.size()) * 0.04) - ti\n",
    "        tot_incoming_drive = incoming_drive_i - ti.cuda()\n",
    "        \n",
    "        \"\"\"Implement Heaviside step function\"\"\"\n",
    "\n",
    "        heaviside_step = torch.zeros(len(tot_incoming_drive))\n",
    "\n",
    "        for t in range(len(tot_incoming_drive)):\n",
    "            heaviside_step[t] = torch.Tensor([0.0]).cuda() if tot_incoming_drive[t].cuda() < ti[t].cuda() else torch.Tensor([1.0]).cuda()\n",
    "\n",
    "        yt_next = heaviside_step.unsqueeze(1).cuda()\n",
    "\n",
    "        return yt_next\n",
    "\n",
    "    def recurrent_drive(self, wee, wei, te, x, y,white_noise_e):\n",
    "        \n",
    "        \"\"\"Network state due to recurrent drive received by the each unit at time t+1\"\"\"\n",
    "        \n",
    "    \n",
    "        xt = x[:, 1].unsqueeze(1)  \n",
    "        yt = y[:, 1].unsqueeze(1)\n",
    "        \n",
    "        incoming_drive_e = self.incoming_drive(weights = wee,activity_vector=xt).cuda()\n",
    "        incoming_drive_i = self.incoming_drive(weights = wei,activity_vector=yt).cuda()\n",
    "        \n",
    "        # tot_incoming_drive = incoming_drive_e.add_(torch.randn(incoming_drive_e.size()) * 0.04) -  incoming_drive_i - te\n",
    "        tot_incoming_drive = incoming_drive_e -  incoming_drive_i - te.cuda()\n",
    "        \n",
    "        \"\"\"Heaviside step function\"\"\"\n",
    "\n",
    "        heaviside_step = torch.zeros(len(tot_incoming_drive)).cuda()\n",
    "        for t in range(len(tot_incoming_drive)):\n",
    "            heaviside_step[t] = torch.Tensor([0.0]).cuda() if tot_incoming_drive[t].cuda() < te[t].cuda() else torch.Tensor([1.0]).cuda()\n",
    "\n",
    "        xt_next = heaviside_step.unsqueeze(1).cuda()\n",
    "\n",
    "        return xt_next\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4uxpTRYOlMDZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "class RunSorn(Sorn):\n",
    "    \n",
    "    def __init__(self,phase,matrices,time_steps):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.time_steps = time_steps\n",
    "        Sorn.time_steps = time_steps\n",
    "        self.phase = phase\n",
    "        self.matrices = matrices\n",
    "\n",
    "    def run_sorn(self, inp):\n",
    "        \n",
    "        # Initialize/Get the weight, threshold matrices and activity vectors\n",
    "        matrix_collection = MatrixCollection(phase = self.phase,matrices = self.matrices)\n",
    "        \n",
    "        # Collect the network activity at all time steps\n",
    "        \n",
    "        x_all = []\n",
    "        Y_all = []\n",
    "        R_all = []\n",
    "        \n",
    "        frac_pos_active_conn = []\n",
    "        \n",
    "        # To get the last activation status of Exc and Inh neurons\n",
    "            \n",
    "        for i in tqdm.tqdm(range(self.time_steps)):\n",
    "            \n",
    "            \n",
    "#             \"\"\" Generate white noise\"\"\"\n",
    "#             white_noise_e = white_gaussian_noise(mu= 0., sigma = 0.04,t = Sorn.ne)\n",
    "#             white_noise_i = white_gaussian_noise(mu= 0., sigma = 0.04,t = Sorn.ni)\n",
    "            \n",
    "            network_state = NetworkState(inp)  # Feed input and initialize network state\n",
    "            \n",
    "            # Buffers to get the resulting x and y vectors at the current time step and update the master matrix\n",
    "\n",
    "            x_buffer, y_buffer = torch.zeros(( Sorn.ne, 2)), torch.zeros((Sorn.ni, 2)).cuda()\n",
    "\n",
    "            te_buffer, ti_buffer = torch.zeros((Sorn.ne, 1)), torch.zeros((Sorn.ni, 1)).cuda()\n",
    "\n",
    "            # Get the matrices and rename them for ease of reading\n",
    "\n",
    "            Wee, Wei, Wie = matrix_collection.Wee, matrix_collection.Wei, matrix_collection.Wie\n",
    "            Te, Ti = matrix_collection.Te, matrix_collection.Ti\n",
    "            X, Y = matrix_collection.X, matrix_collection.Y\n",
    "            \n",
    "            \n",
    "            # Recurrent drive at t+1 used to predict the next external stimuli\n",
    "            \n",
    "            r = network_state.recurrent_drive(Wee[i], Wei[i], Te[i], X[i], Y[i],white_noise_e=0.).cuda()\n",
    "            \n",
    "            \"\"\" Fraction of active connections between E-E network\"\"\"\n",
    "            frac_pos_active_conn.append((Wee[i] > 0.0).sum())\n",
    "            \n",
    "            \"\"\"Get excitatory states and inhibitory states given the weights and thresholds\"\"\"\n",
    "\n",
    "            # x(t+1), y(t+1)\n",
    "            excitatory_state_xt_buffer = network_state.excitatory_network_state(Wee[i], Wei[i], Te[i], X[i], Y[i],white_noise_e=0.).cuda()\n",
    "\n",
    "            inhibitory_state_yt_buffer = network_state.inhibitory_network_state(Wie[i], Ti[i], X[i],white_noise_i=0.)\n",
    "            \n",
    "            \"\"\" Update X and Y \"\"\"\n",
    "            x_buffer[:, 0] = X[i][:, 1]  # xt -->(becomes) xt_1\n",
    "            x_buffer[:, 1] = excitatory_state_xt_buffer.squeeze()  # New_activation; x_buffer --> xt\n",
    "            \n",
    "\n",
    "            y_buffer[:, 0] = Y[i][:, 1]\n",
    "            y_buffer[:, 1] = inhibitory_state_yt_buffer.squeeze()\n",
    "            \n",
    "            \"\"\"Plasticity phase\"\"\"\n",
    "\n",
    "            plasticity = Plasticity()\n",
    "\n",
    "            # STDP \n",
    "            Wee_t = plasticity.stdp(Wee[i],x_buffer,cutoff_weights = (0.0,1.0)).cuda()\n",
    "              \n",
    "            # Intrinsic plasticity\n",
    "            Te_t = plasticity.ip(Te[i],x_buffer)\n",
    "              \n",
    "            # Structural plasticity\n",
    "            # Wee_t = plasticity.structural_plasticity(Wee_t)      \n",
    "            \n",
    "            # iSTDP \n",
    "            # Wei_t = plasticity.istdp(Wei[i],x_buffer,y_buffer,cutoff_weights = (0.0,1.0))\n",
    "            \n",
    "            # Synaptic scaling Wee\n",
    "            Wee_t = Plasticity().ss(Wee_t)\n",
    "            \n",
    "            # Synaptic scaling Wei\n",
    "            # Wei_t = Plasticity().ss(Wei_t)\n",
    "\n",
    "            \"\"\"Assign the matrices to the matrix collections\"\"\"\n",
    "            matrix_collection.weight_matrix(Wee_t, Wei[i], Wie[i], i)\n",
    "            matrix_collection.threshold_matrix(Te_t, Ti[i], i)\n",
    "            matrix_collection.network_activity_t(x_buffer, y_buffer, i)\n",
    "            \n",
    "            x_all.append(x_buffer[:,1])\n",
    "            Y_all.append(y_buffer[:,1])\n",
    "            R_all.append(r)\n",
    "            \n",
    "   \n",
    "        plastic_matrices = {'Wee':matrix_collection.Wee[-1], \n",
    "                            'Wei': matrix_collection.Wei[-1], \n",
    "                            'Wie':matrix_collection.Wie[-1],\n",
    "                            'Te': matrix_collection.Te[-1], 'Ti': matrix_collection.Ti[-1],\n",
    "                            'X': X[-1], 'Y': Y[-1]}\n",
    "        \n",
    "        return plastic_matrices,x_all,Y_all,R_all,frac_pos_active_conn\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 1/10 [00:08<01:20,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [00:16<01:08,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [00:24<00:59,  8.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [00:32<00:49,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [00:39<00:39,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [00:47<00:31,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [00:55<00:23,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [01:02<00:15,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [01:10<00:07,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:18<00:00,  7.78s/it]\n"
     ]
    }
   ],
   "source": [
    "plastic_matrices,X_all,Y_all,R_all,frac_pos_active_conn = RunSorn(phase = 'Plasticity',matrices = None,time_steps = 10).run_sorn(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SORN09_GPU.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
